# -*- coding: utf-8 -*-
"""Experimen_Skripsi

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uLWOZcVqKA57VF4r3kG1IMxpxWTy7nhk

# **SKRIPSI**

*"Recruitment System: Resume Screening Automation with LLMs"*

 Menjelaskan pipeline eksperimen promting untuk menilai resume dengan job description (JD)



---

# **STAGE 1 : Install Packages and Setup Variables**

### Install dependencies
"""

# Install dependencies
!pip install -q --upgrade pip setuptools
!pip install transformers accelerate bitsandbytes sentence-transformers datasets scikit-learn tqdm

"""### Imports and Configurations"""

import os, re, json, time
from typing import Dict, Any, Tuple
import pandas as pd
import numpy as np
from tqdm.auto import tqdm
import random
random.seed(42)
np.random.seed(42)

# Transformers / pipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline as hf_pipeline

# Dataset
from datasets import load_dataset
from huggingface_hub import login

# Metrics
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix

# Login huggingface
token = "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"
login(token=token)

"""# **STAGE 2 : Dataset**

Hugging Face Dataset (resume + JD + (3 Label))
- resume_text
- job_description_text
- label: (No Fit, Potential Fit, Good Fit)

path dataset: cnamuangtoun/resume-job-description-fit
"""

DATASET = "cnamuangtoun/resume-job-description-fit"

# Load dataset
dataset = load_dataset(DATASET)
dataset

# show dataset
df_test = pd.DataFrame(dataset['test'])
df = pd.DataFrame(dataset['train'])
df.head()

df.info()

"""# **STAGE 3 : Baseline Model**

## LLM

Model yang dipilih terbatas hanya model opensource yang bisa diakses melalui Hugging Face :)


Model:
- meta-llama/Llama-2-7b-chat-hf
- mistralai/Mistral-7B-Instruct-v0.3
- Qwen/Qwen2.5-7B-Instruct
"""

# Load Mistral 7B Instruct (inference pipeline)
MODEL_ID = "mistralai/Mistral-7B-Instruct-v0.3"

# Import BitsAndBytesConfig for optimized loading
from transformers import BitsAndBytesConfig

# Attempt optimized load; fallback to normal
try:
    # Define BitsAndBytesConfig for 4-bit quantization
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype="float16",
        bnb_4bit_use_double_quant=False,
    )
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        quantization_config=bnb_config, # Pass the BitsAndBytesConfig
        device_map="auto",
        torch_dtype="auto"
    )
    llm_pipe = hf_pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        device_map="auto",
        max_new_tokens=256,
        temperature=0.0,
        do_sample=False
    )
    print("Mistral loaded with optimized settings.")
except Exception as e:
    print("Optimized load failed (Colab environment may not support). Trying normal load... ", e)
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        torch_dtype="auto",
        device_map="auto"
    )
    llm_pipe = hf_pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        device_map="auto",
        max_new_tokens=256,
        temperature=0.0,
        do_sample=False
    )
    print("Mistral loaded (fallback).")

"""# **STAGE 4: Utility Function**

## LLM call wrapper
Option:
1. full prompt
2. with System & user prompt (prefer this)
"""

def call_llm(prompt_text, max_retries=2, sleep=1.0):
    """Call the LLM pipeline and return text output."""
    for attempt in range(max_retries):
        try:
            # Ensure only generated text is returned, not the full prompt
            out = llm_pipe(prompt_text, return_full_text=False)
            # hf_pipeline text-generation returns list of dicts
            txt = out[0].get("generated_text", out[0].get("text", ""))
            return txt.strip()
        except Exception as e:
            print("LLM call error:", e, "attempt", attempt+1)
            time.sleep(sleep)
    raise RuntimeError("LLM failed after retries")

def chat_llm(system, user, max_tokens=512):
    messages = [
        {"role": "system", "content": system},
        {"role": "user", "content": user}
    ]

    # Template building
    tokenized = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to(model.device)

    output = model.generate(
        tokenized,
        max_new_tokens=max_tokens,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id,
        do_sample=False,
        temperature=0.0
    )

    decoded = tokenizer.decode(output[0], skip_special_tokens=True)

    # Remove echoes if any
    if user in decoded:
        decoded = decoded.split(user)[-1].strip()

    return decoded.strip()

"""## Parsing Helpers"""

# Parser for multi-aspect scoring: extract Score and Reasoning
def parse_scores(text):
    score_mapping = {
        "c_skill": "Skill_Match",
        "c_experience": "Experience_Alignment",
        "c_domain": "Domain_Fit",
        "c_tools": "Tool_Tech_Match",
        "c_seniority": "Seniority_Alignment",
        "c_education": "Education_Alignment"
    }
    scores = {}

    def get(label):
        m = re.search(rf"{label}\s*:\s*(\d{{1,3}})", text)
        return int(m.group(1)) if m else 0

    for result_key, source_key in score_mapping.items():
        scores[result_key] = get(source_key)

    # Reasoning
    m = re.search(r"Overall_Reasoning\s*:\s*(.*)", text, re.DOTALL)
    reason = m.group(1).strip() if m else ""

    return scores, reason

"""## LAYER 1: LLM Extraction Function"""

def extract_resume(resume_text, threshold = 1600):
    user_prompt = f"""{RESUME_EXTRACTION_USER}

Resume:
{resume_text}"""

    token_count = len(tokenizer.encode(user_prompt))
    print("Token Count: ", token_count)

    max_tokens = 800
    if token_count > threshold:
        max_tokens *= 2

        if token_count > threshold*2:
            user_prompt += """
The resume is too long (more than {threshold} tokens).
For elements that are likely to be lengthy (such as Core Skills and Work Experience Summary),
first summarize them objectively and in a structured manner, then proceed with the extraction according to the specified format.
"""
    elif token_count > 1000:
        max_tokens = 1024

    return chat_llm(RESUME_EXTRACTION_SYSTEM, user_prompt, max_tokens)

def extract_jd(jd_text):
    user_prompt = f"""{JD_EXTRACTION_USER}

Job Description:
{jd_text}"""
    return chat_llm(JD_EXTRACTION_SYSTEM, user_prompt, 800)

"""## LAYER 2: LLM Multi-Aspect Evaluation Scoring"""

def score_candidate(resume_summary, jd_summary):
    user_p = MULTI_ASPECT_SCORING_USER.format(
        resume_summary=resume_summary,
        jd_summary=jd_summary
    )
    out = chat_llm(MULTI_ASPECT_SCORING_SYSTEM, user_p)
    scores, reason = parse_scores(out)
    return out, scores, reason

"""## LAYER 3: Weighted aggregation & decision mapping

Formula (can ajust):
```
Final Score =
(0.30 * Skill_Match) +
(0.25 * Experience_Alignment) +
(0.15 * Domain_Fit) +
(0.15 * Tool_Tech_Match) +
(0.10 * Seniority_Alignment) +
(0.05 * Education_Alignment)
```



Rule-Based Final Decision:
- Score ‚â• 70 ‚Üí GOOD FIT
- 50 ‚â§ Score < 70 ‚Üí POTENTIAL FIT
- Score < 50 ‚Üí NO FIT
"""

# # weights (tweakable)
# WEIGHTS = {
#     "Skill_Match": 0.30,
#     "Experience_Alignment": 0.30,
#     "Domain_Fit": 0.15,
#     "Tool_Tech_Match": 0.10,
#     "Seniority_Alignment": 0.10,
#     "Education_Alignment": 0.05
# }
WEIGHTS = WEIGHTS
THRESHOLD = THRESHOLD

def aggregate_and_decide(scores, threshold = THRESHOLD, binary = False, weights = WEIGHTS):
    # compute weighted average using available scores
    final_score = 0
    for k,w in weights.items():
        final_score += scores.get(k,0)*w
    final_score = int(round(final_score))

    # decision thresholds
    if final_score >= threshold:
        decision = "Good Fit"
    elif binary and final_score >= 50:
        decision = "Potential Fit"
    else:
        decision = "No Fit"

    return final_score, decision

"""# **STAGE 5: Pipeline Screening System**

## Promting
"""

# RESUME EXTRACTION PROMPT
RESUME_EXTRACTION_SYSTEM = """
You are an HR analyst specializing in professional resume screening.
Your task is to extract structured, factual information from resumes.
If an element is missing, write 'Not Mentioned'.
Provide concise, well-structured output.
"""

RESUME_EXTRACTION_USER = """
Extract the following elements from this resume:

1. Core Skills: List technical or domain skills.
2. Work Experience Summary: Summarize roles, responsibilities, seniority level, and nature of work performed.
3. Domain Expertise: Identify industries or domains the candidate has experience in.
4. Tools & Technologies: List tools, software, platforms, or systems used.
5. Years of Experience: Extract or estimate total relevant experience.
6. Education & Certifications: Highest degree, field, and certifications.
7. Notable Achievements: Measurable or high-impact accomplishments.

Do not invent information. If unsure, respond with 'Not Mentioned'.
Return only the extracted sections.
"""

RESUME_EXTRACTION_PROMPT = f"{RESUME_EXTRACTION_SYSTEM}\n\n{RESUME_EXTRACTION_USER}"

# JD EXTRACTION PROMPT
JD_EXTRACTION_SYSTEM = """
You are an HR analyst specializing in job requirement analysis.
Your task is to extract structured requirements from job descriptions.
If a requirement is missing, write 'Not Mentioned'.
Provide concise, well-structured output.
"""

JD_EXTRACTION_USER = """
Extract the following elements from this job description:

1. Mandatory Skills: Skills explicitly required for the role.
2. Preferred Skills: Skills described as preferred or 'nice-to-have'.
3. Core Responsibilities: Summarize main tasks of the role.
4. Domain Requirements: Industry or functional domain.
5. Tools & Technologies: Tools or platforms required/preferred.
6. Required Experience Level: Identify seniority and years if mentioned.
7. Education Requirements: Minimum degree or field.

Do not invent information.
Return only the extracted sections.
"""

JD_EXTRACTION_PROMPT = f"{JD_EXTRACTION_SYSTEM}\n\n{JD_EXTRACTION_USER}"

MULTI_ASPECT_SCORING_SYSTEM = """
You are a Expert Talent Acquisition Specialist and Technical Recruiter with expertise in competency-based evaluation.
Your task is to objectively evaluate a candidate's fit against a Job Description (JD) based strictly on the provided information.

### CORE EVALUATION PROTOCOLS:
1.  **SEMANTIC MATCHING (CRITICAL):** - Do NOT behave like a simple keyword scanner. You must recognize industry-standard synonyms and concept equivalents.
    - *Example:* If JD asks for "B2B Sales" and Resume shows "Enterprise Account Management" with revenue targets, this is a STRONG MATCH.
    - *Example:* If JD asks for "Python" and Resume lists "Django/Flask development", this is a MATCH.
2.  **STRICT EVIDENCE RULE:**
    - You may translate synonyms, but you CANNOT hallucinate skills.
    - If a skill is not mentioned and cannot be logically inferred from a specific tool/role (e.g., "Excel" implies "Data Entry" but NOT "VBA Macros"), treat it as MISSING.
    - If JD does not mention an aspect requirement, NEUTRAL 50.
3.  **SCORING SCALE:**
    - Scores must be integers between 0-100.
    - Be objective. A "Perfect Match" (90-100) is rare. A "Good Fit" is usually 75-89.
"""

MULTI_ASPECT_SCORING_USER = """
## TASK:
Evaluate the Candidate resume based on the Job Description using the scoring rubric below.

## SCORING RUBRIC (STRICT ANCHORS):
### 1. SKILL MATCH (Technical & Soft Skills)
* 90-100: Matches ALL "Must-Have" skills + Most "Preferred" skills.
* 75-89: Matches ALL "Must-Have" skills + Some "Preferred" skills.
* 60-74: Matches MOST "Must-Have" skills (Key core skills present), but misses some minor ones.
* 40-59: Misses one or more CRITICAL "Must-Have" skills.
* 0-39: Lacks core required skills.

### 2. EXPERIENCE ALIGNMENT (Depth & Responsibilities)
* 90-100: Exceeds years of experience AND responsibility scope is identical or larger.
* 75-89: Meets years of experience AND responsibility scope is highly relevant.
* 60-74: Slightly under years OR responsibility is related but less complex.
* 40-59: Significant gap in years (<50% required) OR role is too junior/different focus.
* 0-39: Irrelevant background.

### 3. DOMAIN FIT (Industry & Business Context)
* 90-100: Exact industry match (e.g., Fintech to Fintech).
* 70-89: Adjacent/Related industry (e.g., E-commerce to Fintech, both are digital).
* 50-69: Different industry but transferable business logic (e.g., Traditional Banking to Fintech).
* 0-49: Completely unrelated industry with no clear transferability.

### 4. TOOLS & TECHNOLOGY (Specific Stack/Software)
* 90-100: Expert proficiency in primary stack/tools.
* 70-89: Good familiarity with primary stack.
* 40-69: Familiar with alternative tools (e.g., knows AWS but JD asks for Azure) OR basic knowledge only.
* 0-39: No familiarity with required tools.

### 5. SENIORITY MATCH (Level of Role)
* 90-100: Perfect level match (e.g., Senior to Senior).
* 70-89: One level difference but adaptable (e.g., Mid-Senior to Senior).
* 40-69: Significant level gap (e.g., Junior to Senior, or VP to Staff).
* 0-39: Complete mismatch.

### 6. EDUCATION (Degree & Certifications)
* 90-100: Exact degree/certifications match.
* 60-89: Related degree or equivalent practical experience (if JD allows).
* 0-59: Missing required degree or unrelated field.

## EVALUATION DATA:
Resume Summary:
{resume_summary}

Job Description Summary:
{jd_summary}

## OUTPUT FORMAT (STRICT):
Aspect_Scores:
Skill_Match: <0-100>
Experience_Alignment: <0-100>
Domain_Fit: <0-100>
Tool_Tech_Match: <0-100>
Seniority_Alignment: <0-100>
Education_Alignment: <0-100>

Overall_Reasoning: <A short 1‚Äì3 sentence evidence-based explanation. Summarize the candidate's strongest alignment areas and critical gaps>
"""

MULTI_ASPECT_SCORING_PROMPT = f"{MULTI_ASPECT_SCORING_SYSTEM}\n\n{MULTI_ASPECT_SCORING_USER}"

MULTI_ASPECT_SCORING_SYSTEM = """
You are an HR evaluation expert specializing in competency-based resume screening.
Your task is to assign objective 0‚Äì100 scores for how well a candidate aligns with a job description.

## EVALUATION PHILOSOPHY:
1. **Strict Evidence-Based**: Score ONLY on explicitly mentioned information. No assumptions.
2. **Must-Have First**: If a must-have requirement is missing, score that aspect ‚â§40.
3. **No Neutral Inflation**: If JD doesn't mention an aspect ‚Üí NEUTRAL 50.
4. **Veto Rules Apply**:
   - Missing critical must-have skill ‚Üí Skill_Match ‚â§40
   - Experience < 50% of required years ‚Üí Experience_Alignment ‚â§50
   - Complete domain mismatch ‚Üí Domain_Fit ‚â§30
"""

MULTI_ASPECT_SCORING_USER = """
# SCORING FRAMEWORK WITH STRICT ANCHORS

## VETO RULES (CRITICAL):
- If a MUST-HAVE requirement is explicitly stated in JD but NOT in resume ‚Üí MAX 40 for that aspect.
- If experience years are less than 50% of required ‚Üí MAX 50 for Experience_Alignment.
- If domain is completely unrelated (e.g., Arts ‚Üí Engineering) ‚Üí MAX 30 for Domain_Fit.

## ASPECT-SPECIFIC SCORING GUIDELINES:
### 1. SKILL MATCH
- 90-100: ALL must-have + ‚â•80% preferred + extras
- 80-89: ALL must-have + ‚â•50% preferred
- 70-79: ALL must-have skills
- 60-69: 80-99% of must-have skills
- 50-59: 60-79% of must-have skills
- 40-49: 40-59% of must-have skills
- 30-39: 20-39% of must-have skills
- 20-29: 1-19% of must-have skills
- 10-19: Only preferred skills, no must-have
- 0-9: No relevant skills

### 2. EXPERIENCE/RESPONSIBILITIES MATCH
- 90-100: Same role, more experience, exceeds responsibilities
- 80-89: Same role, meets experience, 80%+ responsibility match
- 70-79: Similar role, meets experience, 60%+ responsibility match
- 60-69: Related role, meets 80%+ experience years
- 50-59: Related role, meets 50-79% experience years
- 40-49: Different role, some transferable experience
- 30-39: Limited relevant experience
- 20-29: Very limited experience
- 10-19: Entry-level for senior role
- 0-9: No relevant experience

### 3. DOMAIN FIT
- 90-100: Exact industry + niche
- 80-89: Same industry, different niche
- 70-79: Closely related industry
- 60-69: Related industry with transferable skills
- 50-59: Different industry but some overlap
- 40-49: Different industry, minimal overlap
- 30-39: Major mismatch
- 20-29: Completely unrelated
- 10-19: Extreme mismatch
- 0-9: Contradictory domains

### 4. TOOLS & TECHNOLOGY MATCH
- 90-100: All required tools + advanced proficiency
- 80-89: All required tools at working level
- 70-79: ‚â•80% required tools
- 60-69: 60-79% required tools
- 50-59: 40-59% required tools
- 40-49: 20-39% required tools
- 30-39: 1-19% required tools
- 20-29: No required tools but similar ones
- 10-19: Completely different toolset
- 0-9: No tools mentioned

### 5. SENIORITY MATCH
- 90-100: Exact match + more experience
- 80-89: Exact match
- 70-79: One level difference (e.g., Mid ‚Üí Senior)
- 60-69: Two levels difference with compensation
- 50-59: Significant mismatch but plausible
- 40-49: Major mismatch (Junior ‚Üí Senior)
- 30-39: Extreme mismatch
- 20-29: Completely inappropriate
- 0-19: No alignment

### 6. EDUCATION BACKGROUND
- 90-100: Exact degree + required certifications
- 80-89: Exact degree + some certifications
- 70-79: Exact degree, no certifications
- 60-69: Closely related degree
- 50-59: Somewhat related degree
- 40-49: Unrelated but technical degree
- 30-39: Non-technical degree for technical role
- 20-29: No degree when required
- 10-19: Education irrelevant
- 0-9: No education mentioned

## EVALUATION DATA:
Resume Summary:
{resume_summary}

Job Description Summary:
{jd_summary}

## OUTPUT FORMAT (STRICT):
Aspect_Scores:
Skill_Match: <0-100>
Experience_Alignment: <0-100>
Domain_Fit: <0-100>
Tool_Tech_Match: <0-100>
Seniority_Alignment: <0-100>
Education_Alignment: <0-100>

Overall_Reasoning: <A short 1‚Äì3 sentence evidence-based explanation. Summarize the candidate's strongest alignment areas, critical gaps,>
"""

MULTI_ASPECT_SCORING_SYSTEM = """
You are an HR evaluation expert specializing in competency-based resume screening.
Your task is to assign objective 0‚Äì100 scores for how well a candidate aligns with a job description requirements.
Base your decision ONLY on the extracted resume and extracted job description.
Do not invent facts beyond the text. If evidence is missing, treat it as a gap.
If an element job description does NOT mention a requirement, do NOT penalize the candidate. give neutral score
Keep the evaluation consistent, factual, and evidence-based.
"""


MULTI_ASPECT_SCORING_USER = """
Evaluate the candidate based on their extracted resume information and extracted job description requirements.

Score the following aspects from 0‚Äì100:
- Skill Match: Check that the candidate explicitly states (or clearly implies) the required technical skills.
- Responsibilities Match: Assess whether prior roles align with the specific responsibilities in the job description.
- Domain / Industry Fit: Whether the candidate has experience in same or a related industries or environments,  ensuring familiarity with market and challenges.
- Tools & Technology Match: Alignment between tools/technologies used by the candidate and tools required in the job.
- Seniority Match / Experience Duration: Evaluate how long the candidate has held relevant roles matches the posting.
- Educational / Professional Background: Degree/certification alignment with job requirements.

Rules:
- score 100 means it has met the requirements
- Higher scores reflect stronger alignment with job expectations.
- Lack of explicit mention is a gap, not an automatic failure.
- Related or adjacent experience still contributes positively.

Then provide:
- A short 1‚Äì3 sentence evidence-based explanation.
- Summarize the candidate's strongest alignment areas, critical gaps,
- Keep it factual and concise.

Evaluation data:
Resume Summary:
{resume_summary}

Job Description Summary:
{jd_summary}

Output format (strict):
Aspect_Scores:
Skill_Match: <0-100>
Experience_Alignment: <0-100>
Domain_Fit: <0-100>
Tool_Tech_Match: <0-100>
Seniority_Alignment: <0-100>
Education_Alignment: <0-100>

Overall_Reasoning: <text>
"""

MULTI_ASPECT_SCORING_PROMPT = f"{MULTI_ASPECT_SCORING_SYSTEM}\n\n{MULTI_ASPECT_SCORING_USER}"

# EVALUATION & SCORING PROMPT
EVALUATION_SYSTEM = """
Yo
Base your evaluation strictly on the extracu are an HR evaluation expert.
Your task is to assess how well a candidate fits a job role using professional HR criteria.ted information.
Do not invent facts.
"""

EVALUATION_USER = """
Resume Summary:
{resume_summary}

Job Description Summary:
{jd_summary}

Using the extracted resume summary and job description summary, provide:

Score: (0-100). Consider skills and tools alignment, relevance of experience and responsibilities, domain match, seniority alignment, and education.
Decision: No Fit (score: 0-69), Good Fit (score: 70-100).
Reason: (3-6 sentences). strongest alignment areas, critical gaps, and justification for the score.
"""

SCORING_PROMPT = """
...
...
...

Rules:
- Only score based on information explicitly stated.
- Higher score means stronger match.
- Be strict but fair.

Output format (strict):
[COMPONENT_SCORES]
Skill: <0-100>
Experience: <0-100>
Education: <0-100>
Industry: <0-100>
Seniority: <0-100>
"""

"""## LLM Extraction - LLM Multi-Aspect Evaluation - Weighted Aggregation & Rule-Based Decision

3 Layer Pipeline
"""

# Evaluate one pair end-to-end
def evaluate_pair(resume_text, jd_text, extract = True, binary = False):

    r = resume_text
    j = jd_text

    if extract:
        print("\nRunning Resume extraction...")
        r = extract_resume(resume_text)
        print("\nRunning JD extraction...")
        j = extract_jd(jd_text)

    print("\nRunning Evaluation scoring...")
    raw, scores, reason = score_candidate(r, j)
    final_score, decision = aggregate_and_decide(scores, binary)
    # scores = apply_domain_rules(scores)  # Apply rules
    # final_score, decision = smart_aggregate_with_rules(scores)

    return {
        "resume_summary": r,
        "jd_summary": j,
        "scores": scores,
        "overall_reason": reason,
        "final_score": final_score,
        "decision": decision,
        "raw_output": raw
    }

"""## LLM Evaluation (with no aggregation & rule-based decision)

2 Layer Pipeline

LLM Extraction - LLM Evaluation (Scoring, Decision, Reasoning)

"""

def evaluate_fit(resume_summary: str, jd_summary: str) -> str:
    user_prompt = EVALUATION_USER.format(
        resume_summary=resume_summary,
        jd_summary=jd_summary
    )
    return chat_llm(EVALUATION_SYSTEM, user_prompt)

"""## LLM Evaluation (with no LLM extraction & aggregation ...)

1 Layer Pipeline

LLM Evaluation (Scoring, Decision, Reasoning)
"""

PROMPT = """
You are an HR screening engine following professional ATS standards.
Your task is to evaluate how well a candidate's resume matches a job description.

Strict rules:
1. If the resume domain is unrelated to the job description (e.g., mechanical vs data engineering), Score must be 0‚Äì30 and Decision must be "Rejected".
2. If the resume lacks any required core skill listed in the job description, Score must be below 50 and Decision must be "Rejected".
3. Never hallucinate skills not explicitly written.
4. Evaluate using five HR criteria:
   - Core Skill Match
   - Required Tools/Technology Match
   - Experience/Years Alignment
   - Role & Industry Relevance
   - Minimum Requirement Satisfaction
5. Penalize missing required skills heavily.
6. Be objective and strict. Do not be optimistic.

Resume:
{resume}

Job Description:
{jd}

Output format (stric):
Score: <0-100>
Decision: <No Fit, Good Fit>
Reason: <1-2 sentences only>

"""

def screen_resume(resume, job_desc):
    # resume_sum = clean_and_summarize(resume)
    # jd_sum = clean_and_summarize(job_desc)

    prompt = PROMPT.format(
        resume=resume,
        jd=job_desc
    )
    output = llm_pipe(prompt, return_full_text=False)[0]["generated_text"]
    return output

# Helper for 2-layer pipeline output parsing
def parse_evaluate_fit_output(text):
    score = 0
    decision = "No Fit"
    reason = ""

    score_match = re.search(r"Fit Score:\s*(\d+)", text)
    if score_match:
        score = int(score_match.group(1))

    category_match = re.search(r"Fit Category:\s*(.*)", text)
    if category_match:
        cat = category_match.group(1).strip()
        if "Good Fit" in cat:
            decision = "Good Fit"
        elif "Potential Fit" in cat:
            decision = "Potential Fit"
        else:
            decision = "No Fit"

    explanation_match = re.search(r"Explanation:\s*(.*)", text, re.DOTALL)
    if explanation_match:
        reason = explanation_match.group(1).strip()

    return {"final_score": score, "decision": decision, "overall_reason": reason}

# Helper for 1-layer pipeline output parsing
def parse_screen_resume_output(text):
    score = 0
    decision = "No Fit"
    reason = ""

    score_match = re.search(r"Score:\s*(\d+)", text)
    if score_match:
        score = int(score_match.group(1))

    decision_match = re.search(r"Decision:\s*(.*)", text)
    if decision_match:
        dec = decision_match.group(1).strip()
        if "Good Fit" in dec:
            decision = "Good Fit"
        elif "Potential Fit" in dec:
            decision = "Potential Fit"
        else:
            decision = "No Fit"

    reason_match = re.search(r"Reason:\s*(.*)", text, re.DOTALL)
    if reason_match:
        reason = reason_match.group(1).strip()

    return {"final_score": score, "decision": decision, "overall_reason": reason}

"""# **STAGE 6: RUN & DEMOS**"""

def test_prompt_improvement():
    """Test prompt baru vs lama dengan 10 samples"""

    # Asumsi df_res adalah DataFrame hasil sebelumnya dengan kolom:
    # 'true_label', 'pred_label', 'final_score'

    samples = df_res.sample(30, random_state=42)

    results_new = []

    for idx, row in samples.iterrows():
        r = row["resume_extract"]
        j = row["jd_extract"]

        # Pakai prompt baru + rules
        print(f"\nProcessing sample {idx+1}/10...")
        try:
            raw, new_scores, reason = score_candidate(r, j)
            # new_scores = apply_domain_rules(new_scores)  # Apply rules
            # new_final, new_decision = smart_aggregate_with_rules(new_scores)
            new_final, new_decision = aggregate_and_decide(new_scores)

            results_new.append({
                "true": row["true_label"],
                "pred": new_decision,
                "score": new_final,
                "domain_score": new_scores.get("c_domain", 0),
                "skill_score": new_scores.get("c_skill", 0),
                "exp_score": new_scores.get("c_experience", 0)
            })
        except Exception as e:
            print(f"Error on sample {idx}: {e}")
            results_new.append({
                "true": row["true_label"],
                "pred": "No Fit",
                "score": 0
            })

    # Calculate accuracy - FIXED
    if len(df_res) > 0:
        # Hitung accuracy dari hasil lama (asumsi ada kolom 'pred_label')
        old_acc = (df_res["true_label"] == df_res["c_decision"]).mean()
    else:
        old_acc = 0

    new_acc = sum(1 for r in results_new if r["true"] == r["pred"]) / len(results_new)

    print(f"\n{'='*50}")
    print(f"Old Prompt Accuracy: {old_acc:.1%}")
    print(f"New Prompt + Rules Accuracy: {new_acc:.1%}")

    if old_acc > 0:
        improvement = ((new_acc - old_acc) / old_acc * 100)
        print(f"Improvement: {improvement:+.1f}%")

    # Tampilkan detail
    print(f"\nDetailed Results:")
    for i, res in enumerate(results_new):
        print(f"{i+1}. True: {res['true']}, Pred: {res['pred']}, Score: {res['score']:.1f}")

    return pd.DataFrame(results_new)


new_res = test_prompt_improvement()

def extract_resume_full(resume_text, max_token = 1024):
    """Extract resume dengan FULL text, tanpa batasan"""
    user_prompt = f"""{RESUME_EXTRACTION_USER}

Resume:
{resume_text}"""

    # Adaptive token limit based on text length
    token_count = len(tokenizer.encode(user_prompt))
    if token_count > max_token:
        max_tokens = min(2048, token_count)

        if token_count > 3000:
            user_promt = user_promt + """
Please resume first"""
    print(f"{token_count} {max_tokens}")

    return chat_llm(RESUME_EXTRACTION_SYSTEM, user_prompt, max_tokens)

sample_idx = 5678
resume_extract = extract_resume_full(df.iloc[sample_idx]["resume_text"])
print(resume_extract)

sample_idx = 0
sample = df.iloc[sample_idx]
resume = sample["resume_extract"]
jd = sample["jd_extract"]
label = sample["true_label"]

print("Resume:", resume)
print("\nJD:", jd)
print("\nLabel:", label)

"""## 3 Layer Pipeline"""

sample_idx = 10
sample = df.iloc[sample_idx]
resume = sample["resume_extract"]
jd = sample["jd_extract"]

res = evaluate_pair(resume, jd, extract = False, binary = True)

print("\n=== Aspect breakdown ===")
print(res['raw_output'])

print("\nFinal Score:", res["final_score"])
print("Decision:", res["decision"])

print("\n=== Evaluation Result Sample ===")
print("True Label      :", sample['true_label'])
print("Predicted Label :", sample['c_decision'])
print("Final Score     :", sample['c_final_score'])
print("Overall Reason  :", sample['c_reason'])

print("\n--- Aspect Scores ---")
print("Skill Match            :", sample['c_skill'])
print("Experience Alignment   :", sample['c_experience'])
print("Domain Fit             :", sample['c_domain'])
print("Tool & Tech Match      :", sample['c_tools'])
print("Seniority Alignment    :", sample['c_seniority'])
print("Education Alignment    :", sample['c_education'])


print("\n\nResume Summary:\n", sample["resume_extract"])
print("\nJD Summary:\n", sample["jd_extract"])

"""## 2 Layer Pipeline"""

print("\nRunning evaluation...")
eval_out = evaluate_fit(res["resume_summary"], res["jd_summary"])
print("\nEVALUATION OUTPUT\n", eval_out)

print(res['scores'])

"""## 1 Layer Pipeline"""

result = screen_resume(resume, jd)
print(result)

"""---

# **STAGE  7: Evaluation**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import json

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings('ignore')

"""## Setup Path, Configuration"""

# Set style
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (12, 8)

from google.colab import drive
drive.mount('/content/drive')

ROOT_DIR = "/content/drive/MyDrive/skripsi/hpc/"
ROOT_DIR

RESULTS_FILE = "pipeline_c_modified_20251208_031248.csv"
RESULTS_DIR = ROOT_DIR + RESULTS_FILE

CLEANED_FILE = "cleaned_pipeline_results.csv"
CLEANED_DIR = ROOT_DIR + CLEANED_FILE

CONFIG_FILE = "pipeline_c_final_config.json"
CONFIG_DIR = ROOT_DIR + CONFIG_FILE

# Attempt to load and overwrite with optimized config
with open(CONFIG_DIR, 'r') as f:
    config = json.load(f)

opt_config = config['optimized_config']

# Current weights
WEIGHTS = opt_config['weights']
THRESHOLD = opt_config['threshold']

print("Weights: ", WEIGHTS)
print("Threshold:", THRESHOLD)

WEIGHTS = {'c_skill': 0.30, 'c_experience': 0.30,
           'c_domain': 0.15, 'c_tools': 0.10,
           'c_seniority': 0.10, 'c_education': 0.05}
THRESHOLD = 60

"""##Load & Prepare Data"""

# ============================================================
# 1. LOAD AND PREPARE DATA
# ============================================================

print("üìä Loading evaluation results...")
df = pd.read_csv(RESULTS_DIR)

print(f"üìà Dataset shape: {df.shape}")
print(f"üî¢ Total samples: {len(df)}")

# Check columns
print("\nüìã Available columns:")
print(df.columns.tolist())

"""### After clean error dataset"""

print("üìä Loading evaluation results...")
df = pd.read_csv(CLEANED_DIR)

print(f"üìà Dataset shape: {df.shape}")
print(f"üî¢ Total samples: {len(df)}")

"""## Data Cleaning, Preprocessing, & Normalization"""

# ============================================================
# 2. DATA CLEANING AND PREPROCESSING
# ============================================================
def clean_data(df):
    """Clean and preprocess the data"""

    # Create binary columns for predictions
    df['a_correct'] = (df['a_decision'] == df['true_label']).astype(bool)
    df['b_correct'] = (df['b_decision'] == df['true_label']).astype(bool)
    df['c_correct'] = (df['c_decision'] == df['true_label']).astype(bool)

    # Create score difference columns
    df['a_score_diff'] = abs(df['a_score'] - df['c_final_score'])
    df['b_score_diff'] = abs(df['b_score'] - df['c_final_score'])

    # Create pipeline comparison columns
    df['ab_agree'] = (df['a_decision'] == df['b_decision']).astype(int)
    df['ac_agree'] = (df['a_decision'] == df['c_decision']).astype(int)
    df['bc_agree'] = (df['b_decision'] == df['c_decision']).astype(int)
    df['all_agree'] = ((df['ab_agree'] == 1) & (df['ac_agree'] == 1)).astype(int)

    # Calculate reason lengths
    df['a_reason_len'] = df['a_reason'].str.len()
    df['b_reason_len'] = df['b_reason'].str.len()
    df['c_reason_len'] = df['c_reason'].str.len()

    # Extract summary lengths
    df['resume_extract_len'] = df['resume_extract'].str.len()
    df['jd_extract_len'] = df['jd_extract'].str.len()

    return df

df_clean = clean_data(df)

print("\nüßπ Data cleaning completed!")
print(f"‚úÖ Added {len(df_clean.columns) - len(df.columns)} new features")

"""## Basic Metrics Calculation"""

# ============================================================
# 3. BASIC METRICS CALCULATION
# ============================================================
def calculate_detailed_metrics(df):
    """Calculate detailed metrics for each pipeline"""

    metrics = []
    pipelines = ['a', 'b', 'c']

    for pipe in pipelines:
        decision_col = f'{pipe}_decision'
        score_col = f'{pipe}_score' if pipe != 'c' else f'{pipe}_final_score'
        correct_col = f'{pipe}_correct'

        # Accuracy
        accuracy = df[correct_col].mean()

        # For "Good Fit" as positive class
        tp = ((df['true_label'] == 'Good Fit') & (df[decision_col] == 'Good Fit')).sum()
        fp = ((df['true_label'] == 'No Fit') & (df[decision_col] == 'Good Fit')).sum()
        tn = ((df['true_label'] == 'No Fit') & (df[decision_col] == 'No Fit')).sum()
        fn = ((df['true_label'] == 'Good Fit') & (df[decision_col] == 'No Fit')).sum()

        # Precision, Recall, F1
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

        # Score statistics
        score_mean = df[score_col].mean()
        score_std = df[score_col].std()
        score_median = df[score_col].median()

        # Confidence (based on score distance from threshold)
        if pipe != 'c':
            confidence = ((df[score_col] - 50) / 50).abs().mean()
        else:
            confidence = ((df[score_col] - THRESHOLD) / 100-THRESHOLD).abs().mean()

        metrics.append({
            'Pipeline': f'Pipeline {pipe.upper()}',
            'Accuracy': accuracy,
            'Precision': precision,
            'Recall': recall,
            'F1_Score': f1,
            'TP': tp,
            'FP': fp,
            'TN': tn,
            'FN': fn,
            'Score_Mean': score_mean,
            'Score_Std': score_std,
            'Score_Median': score_median,
            'Confidence': confidence
        })

    metrics_df = pd.DataFrame(metrics)

    # Calculate overall metrics
    overall_accuracy = metrics_df['Accuracy'].mean()
    overall_f1 = metrics_df['F1_Score'].mean()

    print(f"\nüéØ Overall Average Accuracy: {overall_accuracy:.2%}")
    print(f"üèÜ Overall Average F1-Score: {overall_f1:.2%}")

    return metrics_df

metrics_df = calculate_detailed_metrics(df_clean)

print("\nüìä Detailed Metrics:")
print(metrics_df.round(4))

# ============================================================
# 4. VISUALIZATIONS
# ============================================================
print("\nüìà Generating visualizations...")

# 4.1 Accuracy Comparison
fig1, axes1 = plt.subplots(2, 2, figsize=(16, 12))
fig1.suptitle('Pipeline Performance Comparison', fontsize=16, fontweight='bold')

# Accuracy bar plot
accuracy_data = metrics_df[['Pipeline', 'Accuracy', 'Precision', 'Recall', 'F1_Score']]
accuracy_data_melted = accuracy_data.melt(id_vars='Pipeline',
                                          var_name='Metric',
                                          value_name='Value')

sns.barplot(data=accuracy_data_melted, x='Pipeline', y='Value', hue='Metric', ax=axes1[0,0])
axes1[0,0].set_title('Accuracy Metrics Comparison')
axes1[0,0].set_ylabel('Score')
axes1[0,0].set_ylim(0, 1)
axes1[0,0].tick_params(axis='x', rotation=45)
axes1[0,0].legend(title='Metric')

# Confusion matrix heatmaps
for idx, pipe in enumerate(['a', 'b', 'c']):
    row = (idx + 1) // 2
    col = (idx + 1) % 2

    decision_col = f'{pipe}_decision'
    cm = confusion_matrix(df_clean['true_label'], df_clean[decision_col],
                          labels=['No Fit', 'Good Fit'])

    sns.heatmap(cm, annot=True, fmt='d', cmap='YlOrRd',
                xticklabels=['No Fit', 'Good Fit'],
                yticklabels=['No Fit', 'Good Fit'],
                ax=axes1[row, col])
    axes1[row, col].set_title(f'Pipeline {pipe.upper()} Confusion Matrix')
    axes1[row, col].set_xlabel('Predicted')
    axes1[row, col].set_ylabel('Actual')

plt.tight_layout()
plt.show()

# 4.2 Score Distributions
fig2, axes2 = plt.subplots(2, 3, figsize=(18, 10))
fig2.suptitle('Score Distributions and Comparisons', fontsize=16, fontweight='bold')

# Pipeline A scores
sns.histplot(data=df_clean, x='a_score', hue='true_label',
             kde=True, ax=axes2[0,0])
axes2[0,0].axvline(50, color='red', linestyle='--', alpha=0.7, label='Threshold (50)')
axes2[0,0].set_title('Pipeline A Scores')
axes2[0,0].set_xlabel('Score')
axes2[0,0].set_ylabel('Count')
axes2[0,0].legend()

# Pipeline B scores
sns.histplot(data=df_clean, x='b_score', hue='true_label',
             kde=True, ax=axes2[0,1])
axes2[0,1].axvline(50, color='red', linestyle='--', alpha=0.7, label='Threshold (50)')
axes2[0,1].set_title('Pipeline B Scores')
axes2[0,1].set_xlabel('Score')
axes2[0,1].set_ylabel('Count')

# Pipeline C scores
sns.histplot(data=df_clean, x='c_final_score', hue='true_label',
             kde=True, ax=axes2[0,2])
axes2[0,2].axvline(THRESHOLD, color='red', linestyle='--', alpha=0.7, label=f'Threshold ({THRESHOLD})')
axes2[0,2].set_title('Pipeline C Final Scores')
axes2[0,2].set_xlabel('Score')
axes2[0,2].set_ylabel('Count')

# Score comparisons
# A vs B
axes2[1,0].scatter(df_clean['a_score'], df_clean['b_score'],
                   c=pd.Categorical(df_clean['true_label']).codes,
                   cmap='viridis', alpha=0.6)
axes2[1,0].plot([0, 100], [0, 100], 'r--', alpha=0.3)
axes2[1,0].set_xlabel('Pipeline A Score')
axes2[1,0].set_ylabel('Pipeline B Score')
axes2[1,0].set_title('Pipeline A vs Pipeline B Scores')
axes2[1,0].grid(True, alpha=0.3)

# A vs C
axes2[1,1].scatter(df_clean['a_score'], df_clean['c_final_score'],
                   c=pd.Categorical(df_clean['true_label']).codes,
                   cmap='viridis', alpha=0.6)
axes2[1,1].plot([0, 100], [0, 100], 'r--', alpha=0.3)
axes2[1,1].set_xlabel('Pipeline A Score')
axes2[1,1].set_ylabel('Pipeline C Score')
axes2[1,1].set_title('Pipeline A vs Pipeline C Scores')
axes2[1,1].grid(True, alpha=0.3)

# B vs C
axes2[1,2].scatter(df_clean['b_score'], df_clean['c_final_score'],
                   c=pd.Categorical(df_clean['true_label']).codes,
                   cmap='viridis', alpha=0.6)
axes2[1,2].plot([0, 100], [0, 100], 'r--', alpha=0.3)
axes2[1,2].set_xlabel('Pipeline B Score')
axes2[1,2].set_ylabel('Pipeline C Score')
axes2[1,2].set_title('Pipeline B vs Pipeline C Scores')
axes2[1,2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# 4.3 Decision Agreement
fig3, axes3 = plt.subplots(1, 3, figsize=(18, 5))

# Agreement matrix
agreement_data = pd.DataFrame({
    'AB Agreement': df_clean['ab_agree'],
    'AC Agreement': df_clean['ac_agree'],
    'BC Agreement': df_clean['bc_agree'],
    'All Agreement': df_clean['all_agree']
})

agreement_rates = agreement_data.mean()

sns.barplot(x=agreement_rates.index, y=agreement_rates.values, ax=axes3[0])
axes3[0].set_title('Pipeline Decision Agreement Rates')
axes3[0].set_ylabel('Agreement Rate')
axes3[0].set_ylim(0, 1)
axes3[0].tick_params(axis='x', rotation=45)

# Agreement by true label
agreement_by_label = pd.crosstab(df_clean['true_label'], df_clean['all_agree'])
agreement_by_label.plot(kind='bar', ax=axes3[1])
axes3[1].set_title('All-3 Agreement by True Label')
axes3[1].set_xlabel('True Label')
axes3[1].set_ylabel('Count')
axes3[1].legend(['Disagree', 'Agree'])

# Venn diagram-like representation (simplified)
from matplotlib_venn import venn3
try:
    a_set = set(df_clean[df_clean['a_decision'] == 'Good Fit'].index)
    b_set = set(df_clean[df_clean['b_decision'] == 'Good Fit'].index)
    c_set = set(df_clean[df_clean['c_decision'] == 'Good Fit'].index)

    venn3([a_set, b_set, c_set],
          ('Pipeline A', 'Pipeline B', 'Pipeline C'), ax=axes3[2])
    axes3[2].set_title('Good Fit Decisions Overlap')
except ImportError:
    axes3[2].text(0.5, 0.5, 'Install matplotlib_venn for Venn diagram',
                  ha='center', va='center')
    axes3[2].set_title('Venn Diagram (requires matplotlib_venn)')

plt.tight_layout()
plt.show()

# ============================================================
# 5. DETAILED ASPECT ANALYSIS (PIPELINE C)
# ============================================================
if all(col in df_clean.columns for col in ['c_skill', 'c_experience', 'c_domain', 'c_tools', 'c_seniority', 'c_education']):
    print("\nüîç Detailed Aspect Analysis for Pipeline C")

    aspect_cols = ['c_skill', 'c_experience', 'c_domain', 'c_tools', 'c_seniority', 'c_education']
    aspect_names = ['Skill Match', 'Experience Alignment', 'Domain Fit',
                    'Tools & Tech Match', 'Seniority Alignment', 'Education Alignment']

    # Create aspect dataframe
    aspect_df = pd.DataFrame({
        'Aspect': aspect_names,
        'Mean_Score': [df_clean[col].mean() for col in aspect_cols],
        'Std_Score': [df_clean[col].std() for col in aspect_cols],
        'Median_Score': [df_clean[col].median() for col in aspect_cols]
    })

    print("\nüìä Aspect Scores Summary:")
    print(aspect_df.round(2))

    # Visualization
    fig4, axes4 = plt.subplots(2, 2, figsize=(16, 12))

    # Aspect scores bar plot
    aspect_df_sorted = aspect_df.sort_values('Mean_Score', ascending=False)
    sns.barplot(data=aspect_df_sorted, x='Mean_Score', y='Aspect', ax=axes4[0,0])
    axes4[0,0].axvline(50, color='red', linestyle='--', alpha=0.7)
    axes4[0,0].set_title('Mean Aspect Scores (Pipeline C)')
    axes4[0,0].set_xlabel('Mean Score')
    axes4[0,0].set_xlim(0, 100)

    # Aspect scores by decision
    aspect_by_decision = df_clean.groupby('c_decision')[aspect_cols].mean()
    aspect_by_decision.columns = aspect_names
    aspect_by_decision.T.plot(kind='bar', ax=axes4[0,1])
    axes4[0,1].set_title('Aspect Scores by Pipeline C Decision')
    axes4[0,1].set_ylabel('Mean Score')
    axes4[0,1].legend(title='Decision')
    axes4[0,1].tick_params(axis='x', rotation=45)

    # Correlation heatmap
    correlation_matrix = df_clean[aspect_cols + ['c_final_score']].corr()
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm',
                center=0, fmt='.2f', ax=axes4[1,0])
    axes4[1,0].set_title('Aspect Scores Correlation Matrix')

    # Aspect scores distribution
    aspect_data_long = pd.melt(df_clean[aspect_cols], var_name='Aspect', value_name='Score')
    aspect_data_long['Aspect'] = aspect_data_long['Aspect'].map(dict(zip(aspect_cols, aspect_names)))
    sns.boxplot(data=aspect_data_long, x='Aspect', y='Score', ax=axes4[1,1])
    axes4[1,1].axhline(50, color='red', linestyle='--', alpha=0.7)
    axes4[1,1].set_title('Aspect Scores Distribution')
    axes4[1,1].set_xticklabels(axes4[1,1].get_xticklabels(), rotation=45, ha='right')

    plt.tight_layout()
    plt.show()

    weighted_scores = {}
    for aspect, weight in WEIGHTS.items():
        weighted_scores[aspect_names[aspect_cols.index(aspect)]] = df_clean[aspect].mean() * weight

    weighted_df = pd.DataFrame(list(weighted_scores.items()),
                               columns=['Aspect', 'Weighted_Contribution'])
    weighted_df['Percentage'] = weighted_df['Weighted_Contribution'] / weighted_df['Weighted_Contribution'].sum() * 100

    print("\n‚öñÔ∏è Weighted Contributions to Final Score:")
    print(weighted_df.round(2))

    # Pie chart for contributions
    fig5, ax5 = plt.subplots(figsize=(10, 8))
    ax5.pie(weighted_df['Weighted_Contribution'], labels=weighted_df['Aspect'],
            autopct='%1.1f%%', startangle=90)
    ax5.set_title('Weighted Contributions to Pipeline C Final Score')
    plt.show()

# ============================================================
# 8. CORRELATION ANALYSIS
# ============================================================
print("\nüìä Correlation Analysis")

# Select numeric columns for correlation
numeric_cols = ['resume_length', 'jd_length',
                'a_score', 'b_score', 'c_final_score',
                'c_skill', 'c_experience', 'c_domain',
                'c_tools', 'c_seniority', 'c_education',
                'a_correct', 'b_correct', 'c_correct']

# Filter to columns that exist
numeric_cols = [col for col in numeric_cols if col in df_clean.columns]

if numeric_cols:
    correlation_matrix = df_clean[numeric_cols].corr()

    fig7, ax7 = plt.subplots(figsize=(14, 12))
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm',
                center=0, fmt='.2f', square=True, ax=ax7)
    ax7.set_title('Feature Correlation Matrix')
    plt.tight_layout()
    plt.show()

    # Find strongest correlations with accuracy
    print("\nüîó Top Correlations with Accuracy:")
    for pipe in ['a', 'b', 'c']:
        correct_col = f'{pipe}_correct'
        if correct_col in correlation_matrix.columns:
            corr_with_accuracy = correlation_matrix[correct_col].sort_values(ascending=False)
            print(f"\nPipeline {pipe.upper()} Accuracy correlates with:")
            for feat, corr in corr_with_accuracy.iloc[1:6].items():  # Skip self-correlation
                print(f"  {feat}: {corr:.3f}")

"""## Process 1: Error Analysis"""

# ============================================================
# 2. ERROR ANALYSIS - FIXED VERSION
# ============================================================
print("\n" + "="*60)
print("üîç ERROR ANALYSIS - WHERE ALL PIPELINES FAIL")
print("="*60)

# FIX: Check if columns exist before using them
if all(col in df.columns for col in ['a_correct', 'b_correct', 'c_correct']):
    # Find samples where ALL pipelines were wrong
    all_wrong_mask = (~df['a_correct']) & (~df['b_correct']) & (~df['c_correct'])
    all_wrong_samples = df[all_wrong_mask]

    print(f"\n‚ùå Samples where ALL pipelines were wrong: {len(all_wrong_samples)}/{len(df)} ({len(all_wrong_samples)/len(df):.1%})")

    if len(all_wrong_samples) > 0:
        print("\nüìù Details of all-wrong samples:")
        for idx, row in all_wrong_samples.iterrows():
            print(f"\nSample ID: {row['sample_id']}")
            print(f"  True Label: {row['true_label']}")
            print(f"  Predictions: A:{row['a_decision']}({row['a_score']}), "
                  f"B:{row['b_decision']}({row['b_score']}), "
                  f"C:{row['c_decision']}({row['c_final_score']})")
            print(f"  Resume length: {row['resume_length']:,}")
            print(f"  JD length: {row['jd_length']:,}")

            # Show aspect scores for Pipeline C
            if 'c_skill' in row.index:
                print(f"  Aspect Scores: Skill:{row['c_skill']}, Exp:{row['c_experience']}, "
                      f"Domain:{row['c_domain']}, Tools:{row['c_tools']}, "
                      f"Seniority:{row['c_seniority']}, Edu:{row['c_education']}")

    # Find samples where ONLY Pipeline C is wrong
    only_c_wrong = (~df['c_correct']) & (df['a_correct']) & (df['b_correct'])
    only_c_wrong_samples = df[only_c_wrong]

    print(f"\n‚ö†Ô∏è Samples where ONLY Pipeline C is wrong: {len(only_c_wrong_samples)}")

    if len(only_c_wrong_samples) > 0:
        print("\nüìä Analysis of Pipeline C-specific errors:")
        for idx, row in only_c_wrong_samples.iterrows():
            print(f"\nSample {row['sample_id']}: True={row['true_label']}, "
                  f"C Prediction={row['c_decision']}({row['c_final_score']})")
else:
    print("‚ùå Required columns not found. Please check your data.")

# Analyze error patterns
print("\n" + "="*60)
print("üîÑ ERROR PATTERN ANALYSIS")
print("="*60)

# Create error pattern column
def get_error_pattern(row):
    pattern = []
    pattern.append('A' if row['a_decision'] != row['true_label'] else '‚úì')
    pattern.append('B' if row['b_decision'] != row['true_label'] else '‚úì')
    pattern.append('C' if row['c_decision'] != row['true_label'] else '‚úì')
    return ''.join(pattern)

df['error_pattern'] = df.apply(get_error_pattern, axis=1)

print("\nüìä Error Patterns Distribution:")
error_counts = df['error_pattern'].value_counts()
for pattern, count in error_counts.items():
    percentage = count / len(df) * 100
    print(f"  {pattern}: {count} samples ({percentage:.1f}%)")

# ============================================================
# 5. DEEP DIVE INTO ALL-WRONG SAMPLES
# ============================================================
print("\n" + "="*60)
print("üî¨ DEEP DIVE: ANALYZING ALL-WRONG SAMPLES")
print("="*60)

if len(all_wrong_samples) > 0:
    print("\nüìä Characteristics of All-Wrong Samples:")

    # Compare with overall statistics
    print("\nüìà Comparison with Overall Dataset:")

    comparison_stats = pd.DataFrame({
        'Metric': ['Average Resume Length', 'Average JD Length',
                   'Avg C Final Score', 'Avg C Skill', 'Avg C Experience',
                   'Avg C Domain', 'Avg C Tools', 'Avg C Seniority', 'Avg C Education'],
        'All_Wrong': [
            all_wrong_samples['resume_length'].mean(),
            all_wrong_samples['jd_length'].mean(),
            all_wrong_samples['c_final_score'].mean(),
            all_wrong_samples['c_skill'].mean() if 'c_skill' in all_wrong_samples.columns else np.nan,
            all_wrong_samples['c_experience'].mean() if 'c_experience' in all_wrong_samples.columns else np.nan,
            all_wrong_samples['c_domain'].mean() if 'c_domain' in all_wrong_samples.columns else np.nan,
            all_wrong_samples['c_tools'].mean() if 'c_tools' in all_wrong_samples.columns else np.nan,
            all_wrong_samples['c_seniority'].mean() if 'c_seniority' in all_wrong_samples.columns else np.nan,
            all_wrong_samples['c_education'].mean() if 'c_education' in all_wrong_samples.columns else np.nan
        ],
        'Overall': [
            df['resume_length'].mean(),
            df['jd_length'].mean(),
            df['c_final_score'].mean(),
            df['c_skill'].mean() if 'c_skill' in df.columns else np.nan,
            df['c_experience'].mean() if 'c_experience' in df.columns else np.nan,
            df['c_domain'].mean() if 'c_domain' in df.columns else np.nan,
            df['c_tools'].mean() if 'c_tools' in df.columns else np.nan,
            df['c_seniority'].mean() if 'c_seniority' in df.columns else np.nan,
            df['c_education'].mean() if 'c_education' in df.columns else np.nan
        ]
    })

    comparison_stats['Difference'] = comparison_stats['All_Wrong'] - comparison_stats['Overall']
    comparison_stats['Difference_Pct'] = (comparison_stats['Difference'] / comparison_stats['Overall']) * 100

    print(comparison_stats.round(2))

    # Analyze specific patterns in all-wrong samples
    print("\nüéØ Pattern Analysis in All-Wrong Samples:")

    # Check if predictions are consistently wrong in same direction
    all_wrong_samples['all_good_fit'] = (
        (all_wrong_samples['a_decision'] == 'Good Fit') &
        (all_wrong_samples['b_decision'] == 'Good Fit') &
        (all_wrong_samples['c_decision'] == 'Good Fit')
    ).astype(int)

    all_wrong_samples['all_no_fit'] = (
        (all_wrong_samples['a_decision'] == 'No Fit') &
        (all_wrong_samples['b_decision'] == 'No Fit') &
        (all_wrong_samples['c_decision'] == 'No Fit')
    ).astype(int)

    print(f"  All predict 'Good Fit' (false positive): {all_wrong_samples['all_good_fit'].sum()}")
    print(f"  All predict 'No Fit' (false negative): {all_wrong_samples['all_no_fit'].sum()}")
    print(f"  Mixed predictions: {len(all_wrong_samples) - all_wrong_samples['all_good_fit'].sum() - all_wrong_samples['all_no_fit'].sum()}")

    # Visualize all-wrong samples
    fig3, axes3 = plt.subplots(1, 3, figsize=(15, 5))

    # Score distributions
    axes3[0].hist(df[df['true_label'] == 'Good Fit']['c_final_score'],
                  alpha=0.3, label='Good Fit (Overall)', bins=20, density=True)
    axes3[0].hist(df[df['true_label'] == 'No Fit']['c_final_score'],
                  alpha=0.3, label='No Fit (Overall)', bins=20, density=True)
    axes3[0].hist(all_wrong_samples['c_final_score'],
                  alpha=0.7, label='All-Wrong Samples', bins=10, density=True, color='red')
    axes3[0].axvline(THRESHOLD, color='black', linestyle='--', alpha=0.7, label=f'Threshold ({THRESHOLD})')
    axes3[0].set_xlabel('Pipeline C Score')
    axes3[0].set_ylabel('Density')
    axes3[0].set_title('Score Distribution: All-Wrong vs Overall')
    axes3[0].legend()
    axes3[0].grid(True, alpha=0.3)

    # Aspect scores comparison
    if all(col in all_wrong_samples.columns for col in aspect_cols):
        aspect_means_all_wrong = all_wrong_samples[aspect_cols].mean()
        aspect_means_overall = df[aspect_cols].mean()

        aspect_comparison = pd.DataFrame({
            'Aspect': [col.replace('c_', '').title() for col in aspect_cols],
            'All_Wrong': aspect_means_all_wrong.values,
            'Overall': aspect_means_overall.values
        })

        aspect_comparison.plot(x='Aspect', kind='bar', ax=axes3[1])
        axes3[1].set_title('Aspect Scores: All-Wrong vs Overall')
        axes3[1].set_ylabel('Average Score')
        axes3[1].tick_params(axis='x', rotation=45)
        axes3[1].legend(['All-Wrong Samples', 'Overall'])
        axes3[1].grid(True, alpha=0.3, axis='y')

    # Decision patterns
    if 'error_pattern' in all_wrong_samples.columns:
        pattern_counts = all_wrong_samples['error_pattern'].value_counts()
        axes3[2].bar(range(len(pattern_counts)), pattern_counts.values)
        axes3[2].set_xticks(range(len(pattern_counts)))
        axes3[2].set_xticklabels(pattern_counts.index, rotation=45)
        axes3[2].set_xlabel('Error Pattern')
        axes3[2].set_ylabel('Count')
        axes3[2].set_title('Error Patterns in All-Wrong Samples')
        axes3[2].grid(True, alpha=0.3, axis='y')

        # Add value labels
        for i, v in enumerate(pattern_counts.values):
            axes3[2].text(i, v + 0.1, str(v), ha='center')

    plt.tight_layout()
    plt.show()

# ============================================================
# 2. ANALYZE CONFIDENCE LEVELS OF WRONG PREDICTIONS
# ============================================================

print("\n" + "="*60)
print("üìä CONFIDENCE ANALYSIS OF WRONG PREDICTIONS")
print("="*60)

# Identify ALL pipelines wrong
df['all_wrong'] = ((~df['a_correct']) & (~df['b_correct']) & (~df['c_correct'])).astype(int)

all_wrong_samples = df[df['all_wrong'] == 1]
print(f"\n‚ùå ALL PIPELINES WRONG: {len(all_wrong_samples)}/{len(df)} ({len(all_wrong_samples)/len(df):.1%})")


# Calculate confidence scores for each pipeline
def calculate_confidence(score, threshold=65):
    """Calculate how far score is from threshold"""
    return abs(score - threshold)

# Calculate confidence for each pipeline
df['a_confidence'] = df['a_score'].apply(lambda x: calculate_confidence(x, 50))
df['b_confidence'] = df['b_score'].apply(lambda x: calculate_confidence(x, 50))
df['c_confidence'] = df['c_final_score'].apply(lambda x: calculate_confidence(x, THRESHOLD))

# For all-wrong samples, calculate average confidence
all_wrong_samples['avg_confidence'] = all_wrong_samples[['a_confidence', 'b_confidence', 'c_confidence']].mean(axis=1)

# Define confidence thresholds
CONFIDENCE_HIGH = 25  # Score >25 points from threshold = high confidence
CONFIDENCE_MEDIUM = 15  # 15-25 points from threshold = medium confidence
CONFIDENCE_LOW = 5  # <15 points from threshold = low confidence

# Categorize all-wrong samples by confidence
all_wrong_samples['confidence_category'] = pd.cut(
    all_wrong_samples['avg_confidence'],
    bins=[0, CONFIDENCE_LOW, CONFIDENCE_MEDIUM, 100],
    labels=['Low', 'Medium', 'High'],
    include_lowest=True
)

print("\nüîç Confidence Distribution of All-Wrong Samples:")
confidence_counts = all_wrong_samples['confidence_category'].value_counts()
for category, count in confidence_counts.items():
    percentage = count / len(all_wrong_samples) * 100
    print(f"  {category} Confidence: {count} samples ({percentage:.1f}%)")

# ============================================================
# 4. DEEP ANALYSIS OF EACH CATEGORY
# ============================================================
print("\n" + "="*60)
print("üî¨ DEEP ANALYSIS BY CONFIDENCE CATEGORY")
print("="*60)

# Identify which samples to remove
samples_to_remove = all_wrong_samples[all_wrong_samples['confidence_category'] == 'High']
samples_to_review = all_wrong_samples[all_wrong_samples['confidence_category'] == 'Medium']
samples_to_keep = all_wrong_samples[all_wrong_samples['confidence_category'] == 'Low']

print(f"\nüìä Smart Removal Plan:")
print(f"  üóëÔ∏è  TO REMOVE (High Confidence Wrong): {len(samples_to_remove)} samples")
print(f"  ‚ö†Ô∏è  TO REVIEW (Medium Confidence): {len(samples_to_review)} samples")
print(f"  ‚úÖ TO KEEP & OPTIMIZE (Low Confidence): {len(samples_to_keep)} samples")

# Analyze each category
categories = {
    'High (to remove)': samples_to_remove,
    'Medium (to review)': samples_to_review,
    'Low (to keep)': samples_to_keep
}

for category_name, category_df in categories.items():
    if len(category_df) > 0:
        print(f"\nüìà {category_name} - {len(category_df)} samples:")

        # Score statistics
        avg_score = category_df['c_final_score'].mean()
        avg_distance = category_df['avg_confidence'].mean()

        # Direction analysis
        false_positives = len(category_df[
            (category_df['a_decision'] == 'Good Fit') &
            (category_df['b_decision'] == 'Good Fit') &
            (category_df['c_decision'] == 'Good Fit')
        ])

        false_negatives = len(category_df[
            (category_df['a_decision'] == 'No Fit') &
            (category_df['b_decision'] == 'No Fit') &
            (category_df['c_decision'] == 'No Fit')
        ])

        print(f"  Avg Pipeline C Score: {avg_score:.1f}")
        print(f"  Avg Distance from Threshold: {avg_distance:.1f} points")
        print(f"  False Positives: {false_positives}")
        print(f"  False Negatives: {false_negatives}")

# ============================================================
# 5. VISUALIZE CONFIDENCE ANALYSIS
# ============================================================
print("\n" + "="*60)
print("üìä VISUALIZING CONFIDENCE ANALYSIS")
print("="*60)

fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. Confidence distribution
confidence_values = all_wrong_samples['avg_confidence'].values
axes[0,0].hist(confidence_values, bins=20, color='skyblue', edgecolor='black')
axes[0,0].axvline(CONFIDENCE_LOW, color='green', linestyle='--', label=f'Low Threshold ({CONFIDENCE_LOW})')
axes[0,0].axvline(CONFIDENCE_MEDIUM, color='orange', linestyle='--', label=f'Medium Threshold ({CONFIDENCE_MEDIUM})')
axes[0,0].axvline(CONFIDENCE_HIGH, color='red', linestyle='--', label=f'High Threshold ({CONFIDENCE_HIGH})')
axes[0,0].set_xlabel('Average Confidence (Distance from Threshold)')
axes[0,0].set_ylabel('Count')
axes[0,0].set_title('Distribution of Confidence in All-Wrong Samples')
axes[0,0].legend()
axes[0,0].grid(True, alpha=0.3)

# 2. Score vs Confidence scatter
scatter = axes[0,1].scatter(
    all_wrong_samples['c_final_score'],
    all_wrong_samples['avg_confidence'],
    c=pd.Categorical(all_wrong_samples['confidence_category']).codes,
    cmap='viridis',
    alpha=0.6
)
axes[0,1].axvline(THRESHOLD, color='black', linestyle='--', label=f'Pipeline C Threshold ({THRESHOLD})')
axes[0,1].axhline(CONFIDENCE_HIGH, color='red', linestyle='--', alpha=0.5)
axes[0,1].set_xlabel('Pipeline C Score')
axes[0,1].set_ylabel('Confidence (Distance from Threshold)')
axes[0,1].set_title('Score vs Confidence in All-Wrong Samples')
axes[0,1].legend()
axes[0,1].grid(True, alpha=0.3)

# Create custom legend for categories
from matplotlib.patches import Patch
legend_elements = [
    Patch(facecolor='green', alpha=0.6, label='Low Confidence (Keep)'),
    Patch(facecolor='orange', alpha=0.6, label='Medium Confidence (Review)'),
    Patch(facecolor='red', alpha=0.6, label='High Confidence (Remove)')
]
axes[0,1].legend(handles=legend_elements, loc='upper right')

# 3. Category distribution
category_counts = [len(samples_to_keep), len(samples_to_review), len(samples_to_remove)]
category_labels = ['Low (Keep)', 'Medium (Review)', 'High (Remove)']
category_colors = ['green', 'orange', 'red']

axes[1,0].bar(category_labels, category_counts, color=category_colors)
axes[1,0].set_ylabel('Number of Samples')
axes[1,0].set_title('Distribution of All-Wrong Samples by Confidence')
axes[1,0].grid(True, alpha=0.3, axis='y')

# Add value labels
for i, v in enumerate(category_counts):
    axes[1,0].text(i, v + 0.5, str(v), ha='center')

# 4. Score distribution by category
for category, color in zip(['Low', 'Medium', 'High'], ['green', 'orange', 'red']):
    category_data = all_wrong_samples[all_wrong_samples['confidence_category'] == category]
    if len(category_data) > 0:
        axes[1,1].hist(category_data['c_final_score'],
                      alpha=0.5, label=f'{category} Confidence',
                      bins=15, density=True, color=color)

axes[1,1].axvline(THRESHOLD, color='black', linestyle='--', label=f'Threshold ({THRESHOLD})')
axes[1,1].set_xlabel('Pipeline C Score')
axes[1,1].set_ylabel('Density')
axes[1,1].set_title('Score Distribution by Confidence Category')
axes[1,1].legend()
axes[1,1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ============================================================
# 6. CREATE OPTIMIZED DATASETS
# ============================================================
print("\n" + "="*60)
print("üíæ CREATING OPTIMIZED DATASETS")
print("="*60)

# Create datasets
sample_ids_to_remove = set(samples_to_remove['sample_id'].astype(int).tolist())

# 1. Cleaned dataset (remove only high confidence wrong)
cleaned_df = df[~df['sample_id'].isin(sample_ids_to_remove)].copy()
removed_df = df[df['sample_id'].isin(sample_ids_to_remove)].copy()

print(f"\nüìä Dataset Creation:")
print(f"  ‚úÖ Cleaned Dataset: {len(cleaned_df)} samples ({len(cleaned_df)/len(df):.1%})")
print(f"  üóëÔ∏è  Removed (High Confidence Wrong): {len(removed_df)} samples ({len(removed_df)/len(df):.1%})")
print(f"  ‚ö†Ô∏è  For Review (Medium Confidence): {len(samples_to_review)} samples")
print(f"  üîß For Optimization (Low Confidence): {len(samples_to_keep)} samples")

# ============================================================
# 7. OPTIMIZATION POTENTIAL ANALYSIS
# ============================================================
print("\n" + "="*60)
print("üéØ OPTIMIZATION POTENTIAL ANALYSIS")
print("="*60)

# Analyze low confidence wrong samples for optimization potential
if len(samples_to_keep) > 0:
    print(f"\nüîß {len(samples_to_keep)} LOW CONFIDENCE SAMPLES CAN BE OPTIMIZED:")

    # Group by error type
    fp_low_conf = samples_to_keep[
        (samples_to_keep['a_decision'] == 'Good Fit') &
        (samples_to_keep['b_decision'] == 'Good Fit') &
        (samples_to_keep['c_decision'] == 'Good Fit')
    ]

    fn_low_conf = samples_to_keep[
        (samples_to_keep['a_decision'] == 'No Fit') &
        (samples_to_keep['b_decision'] == 'No Fit') &
        (samples_to_keep['c_decision'] == 'No Fit')
    ]

    print(f"  False Positives (predict Good Fit, should be No Fit): {len(fp_low_conf)}")
    print(f"  False Negatives (predict No Fit, should be Good Fit): {len(fn_low_conf)}")

    # Calculate optimal threshold adjustment
    if len(fp_low_conf) > 0:
        avg_fp_score = fp_low_conf['c_final_score'].mean()
        threshold_increase_needed = avg_fp_score - THRESHOLD  # How much to increase threshold
        print(f"  Avg FP score: {avg_fp_score:.1f} ‚Üí Increase threshold by {threshold_increase_needed:+.1f}")

    if len(fn_low_conf) > 0:
        avg_fn_score = fn_low_conf['c_final_score'].mean()
        threshold_decrease_needed = THRESHOLD - avg_fn_score  # How much to decrease threshold
        print(f"  Avg FN score: {avg_fn_score:.1f} ‚Üí Decrease threshold by {threshold_decrease_needed:+.1f}")

    # Weight optimization analysis
    if all(col in samples_to_keep.columns for col in ['c_skill', 'c_experience', 'c_domain', 'c_tools', 'c_seniority', 'c_education']):
        print(f"\n‚öñÔ∏è Weight Optimization Analysis for Low Confidence Samples:")

        aspect_cols = ['c_skill', 'c_experience', 'c_domain', 'c_tools', 'c_seniority', 'c_education']

        # Calculate average aspect scores for false positives vs false negatives
        if len(fp_low_conf) > 0:
            fp_aspect_means = fp_low_conf[aspect_cols].mean()
            print(f"  False Positives avg aspect scores:")
            for col in aspect_cols:
                print(f"    {col.replace('c_', '').title()}: {fp_aspect_means[col]:.1f}")

        if len(fn_low_conf) > 0:
            fn_aspect_means = fn_low_conf[aspect_cols].mean()
            print(f"  False Negatives avg aspect scores:")
            for col in aspect_cols:
                print(f"    {col.replace('c_', '').title()}: {fn_aspect_means[col]:.1f}")

"""### Save Droped Error Over Confidence


"""

# ============================================================
# 8. SAVE OPTIMIZED DATASETS
# ============================================================
print("\n" + "="*60)
print("üíæ SAVING OPTIMIZED DATASETS")
print("="*60)

# 1. Main cleaned dataset (for optimization)
cleaned_df.to_csv(CLEANED_DIR, index=False)
print(f"‚úÖ Smart Cleaned Dataset: {CLEANED_DIR}")

# 2. Removed samples (high confidence wrong)
removed_df.to_csv(f'high_confidence_wrong_removed.csv', index=False)
print(f"‚úÖ High Confidence Wrong (Removed): high_confidence_wrong_removed.csv")

# 3. Samples for manual review
samples_to_review.to_csv(f'medium_confidence_review.csv', index=False)
print(f"‚úÖ Medium Confidence (Review Needed): medium_confidence_review.csv")

# 4. Low confidence samples (optimization targets)
samples_to_keep.to_csv(f'low_confidence_optimize.csv', index=False)
print(f"‚úÖ Low Confidence (Optimization Targets): low_confidence_optimize.csv")

"""## Process 2: Critical Samples"""

# ============================================================
# 2. DEEP ANALYSIS OF CRITICAL SAMPLES
# ============================================================
print("\n" + "="*60)
print("üìä DEEP ANALYSIS OF CRITICAL SAMPLES")
print("="*60)

# Identify critical errors: ALL pipelines wrong
df['all_wrong'] = ((~df['a_correct']) & (~df['b_correct']) & (~df['c_correct'])).astype(int)

critical_samples = df[df['all_wrong'] == 1]
print(f"\n‚ùå CRITICAL ERRORS FOUND: {len(critical_samples)}/{len(df)} ({len(critical_samples)/len(df):.1%})")

if len(critical_samples) > 0:
    # 2.1 Analyze score patterns
    print("\nüî¢ Score Analysis of Critical Samples:")

    # Calculate score statistics
    score_stats = critical_samples[['a_score', 'b_score', 'c_final_score']].describe()
    print("\nScore Statistics for Critical Samples:")
    print(score_stats.round(2))

    # Check score consistency
    critical_samples['score_variance'] = critical_samples[['a_score', 'b_score', 'c_final_score']].std(axis=1)
    print(f"\nAverage score variance in critical samples: {critical_samples['score_variance'].mean():.1f}")

    # 2.2 Analyze text characteristics
    print("\nüìù Text Characteristics of Critical Samples:")

    text_stats = critical_samples[['resume_length', 'jd_length']].describe()
    print("\nText Length Statistics:")
    print(text_stats.round(0))

    # Compare with overall
    overall_text_stats = df[['resume_length', 'jd_length']].describe()
    print("\nComparison with Overall Dataset:")
    print("Critical vs Overall (Resume Length):")
    print(f"  Mean: {critical_samples['resume_length'].mean():.0f} vs {df['resume_length'].mean():.0f}")
    print(f"  Median: {critical_samples['resume_length'].median():.0f} vs {df['resume_length'].median():.0f}")

    # 2.3 Analyze decision patterns
    print("\nüîÑ Decision Patterns in Critical Samples:")

    # Check if all pipelines agree with each other
    critical_samples['all_agree'] = (
        (critical_samples['a_decision'] == critical_samples['b_decision']) &
        (critical_samples['b_decision'] == critical_samples['c_decision'])
    ).astype(int)

    print(f"\nAll pipelines agree with each other: {critical_samples['all_agree'].sum()} samples")

    # Check direction of error
    print("\nüîç Direction of Errors:")
    false_positives = critical_samples[
        (critical_samples['a_decision'] == 'Good Fit') &
        (critical_samples['b_decision'] == 'Good Fit') &
        (critical_samples['c_decision'] == 'Good Fit')
    ]
    false_negatives = critical_samples[
        (critical_samples['a_decision'] == 'No Fit') &
        (critical_samples['b_decision'] == 'No Fit') &
        (critical_samples['c_decision'] == 'No Fit')
    ]

    print(f"  All predict 'Good Fit' (False Positives): {len(false_positives)}")
    print(f"  All predict 'No Fit' (False Negatives): {len(false_negatives)}")
    print(f"  Mixed predictions: {len(critical_samples) - len(false_positives) - len(false_negatives)}")

    # 2.4 Analyze aspect scores for Pipeline C
    aspect_cols = ['c_skill', 'c_experience', 'c_domain', 'c_tools', 'c_seniority', 'c_education']
    if all(col in critical_samples.columns for col in aspect_cols):
        print("\nüéØ Pipeline C Aspect Scores Analysis:")

        aspect_means_critical = critical_samples[aspect_cols].mean()
        aspect_means_overall = df[aspect_cols].mean()

        aspect_comparison = pd.DataFrame({
            'Aspect': [col.replace('c_', '').title() for col in aspect_cols],
            'Critical_Mean': aspect_means_critical.values,
            'Overall_Mean': aspect_means_overall.values,
            'Difference': aspect_means_critical.values - aspect_means_overall.values
        })

        print(aspect_comparison.round(1))

        # Check for extreme aspect scores
        print("\n‚ö†Ô∏è Extreme Aspect Scores in Critical Samples:")
        for col in aspect_cols:
            extreme_low = (critical_samples[col] == 0).sum()
            extreme_high = (critical_samples[col] == 100).sum()
            if extreme_low > 0 or extreme_high > 0:
                print(f"  {col.replace('c_', '').title()}: {extreme_low} zeros, {extreme_high} hundreds")

# ============================================================
# 3. VISUAL ANALYSIS
# ============================================================
print("\n" + "="*60)
print("üìä VISUAL ANALYSIS OF CRITICAL VS NON-CRITICAL")
print("="*60)

fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# 1. Score distributions
axes[0,0].hist(df[df['all_wrong'] == 0]['c_final_score'],
               alpha=0.5, label='Non-Critical', bins=20, density=True)
axes[0,0].hist(critical_samples['c_final_score'],
               alpha=0.5, label='Critical', bins=20, density=True, color='red')
axes[0,0].axvline(THRESHOLD, color='black', linestyle='--', alpha=0.7, label=f'Threshold ({THRESHOLD})')
axes[0,0].set_xlabel('Pipeline C Score')
axes[0,0].set_ylabel('Density')
axes[0,0].set_title('Score Distribution: Critical vs Non-Critical')
axes[0,0].legend()
axes[0,0].grid(True, alpha=0.3)

# 2. Text length comparison
axes[0,1].boxplot([df[df['all_wrong'] == 0]['resume_length'],
                   critical_samples['resume_length']],
                  labels=['Non-Critical', 'Critical'])
axes[0,1].set_ylabel('Resume Length (characters)')
axes[0,1].set_title('Resume Length Comparison')
axes[0,1].grid(True, alpha=0.3, axis='y')

# 3. Score scatter plot
scatter = axes[0,2].scatter(df['resume_length'], df['c_final_score'],
                           c=df['all_wrong'], alpha=0.6, cmap='coolwarm')
axes[0,2].set_xlabel('Resume Length')
axes[0,2].set_ylabel('Pipeline C Score')
axes[0,2].set_title('Resume Length vs Score (Critical in Red)')
plt.colorbar(scatter, ax=axes[0,2], label='Critical (1=Yes)')
axes[0,2].grid(True, alpha=0.3)

# 4. Aspect scores comparison (if available)
if all(col in df.columns for col in aspect_cols):
    aspect_means_non_critical = df[df['all_wrong'] == 0][aspect_cols].mean()
    aspect_means_critical = critical_samples[aspect_cols].mean()

    x = np.arange(len(aspect_cols))
    width = 0.35

    axes[1,0].bar(x - width/2, aspect_means_non_critical.values, width, label='Non-Critical')
    axes[1,0].bar(x + width/2, aspect_means_critical.values, width, label='Critical')
    axes[1,0].set_xticks(x)
    axes[1,0].set_xticklabels([col.replace('c_', '').title() for col in aspect_cols], rotation=45)
    axes[1,0].set_ylabel('Mean Score')
    axes[1,0].set_title('Aspect Scores Comparison')
    axes[1,0].legend()
    axes[1,0].grid(True, alpha=0.3, axis='y')

# 5. Decision patterns
if 'all_wrong' in df.columns:
    decision_patterns = []
    for _, row in df.iterrows():
        # Membuat pola keputusan (G: Good Fit, N: No Fit)
        pattern = f"{row['a_decision'][0]}{row['b_decision'][0]}{row['c_decision'][0]}"
        decision_patterns.append(pattern)

    df['decision_pattern'] = decision_patterns

    # Hitung pola untuk critical vs non-critical
    critical_counts = df[df['all_wrong'] == 1]['decision_pattern'].value_counts()
    non_critical_counts = df[df['all_wrong'] == 0]['decision_pattern'].value_counts()

    # Gabungkan 5 pola teratas dari kedua kelompok
    top_critical_patterns = critical_counts.head(5).index
    top_non_critical_patterns = non_critical_counts.head(5).index

    # Ambil semua pola unik dari Top 5 kedua grup
    combined_patterns = top_critical_patterns.union(top_non_critical_patterns).sort_values()

    # Reindex kedua Series menggunakan pola gabungan, mengisi NaN (nilai yang hilang) dengan 0
    critical_patterns_aligned = critical_counts.reindex(combined_patterns, fill_value=0)
    non_critical_patterns_aligned = non_critical_counts.reindex(combined_patterns, fill_value=0)

    # Pastikan x memiliki ukuran yang benar (sesuai jumlah pola gabungan)
    x = np.arange(len(combined_patterns))
    width = 0.35

    # Plotting dengan data yang sudah diselaraskan
    axes[1,1].bar(x - width/2, non_critical_patterns_aligned.values, width, label='Non-Critical')
    axes[1,1].bar(x + width/2, critical_patterns_aligned.values, width, label='Critical')

    axes[1,1].set_xticks(x)
    # Gunakan indeks gabungan untuk label sumbu X
    axes[1,1].set_xticklabels(combined_patterns, rotation=45, ha='right')

    axes[1,1].set_ylabel('Count')
    axes[1,1].set_title('Top Decision Patterns (Aligned)')
    axes[1,1].legend()
    axes[1,1].grid(True, alpha=0.3, axis='y')

# 6. Pipeline agreement
if 'all_wrong' in df.columns:
    df['pipelines_agree'] = (
        (df['a_decision'] == df['b_decision']) &
        (df['b_decision'] == df['c_decision'])
    ).astype(int)

    agreement_by_critical = df.groupby('all_wrong')['pipelines_agree'].mean()
    axes[1,2].bar(['Non-Critical', 'Critical'], agreement_by_critical.values)
    axes[1,2].set_ylabel('Pipeline Agreement Rate')
    axes[1,2].set_title('Pipeline Agreement by Critical Status')
    axes[1,2].set_ylim(0, 1)
    axes[1,2].grid(True, alpha=0.3, axis='y')

    # Add value labels
    for i, v in enumerate(agreement_by_critical.values):
        axes[1,2].text(i, v + 0.02, f'{v:.2%}', ha='center')

plt.tight_layout()
plt.show()

# ============================================================
# 3. IDENTIFY SUSPICIOUS SAMPLES USING MULTIPLE METRICS
# ============================================================
print("\n" + "="*60)
print("üü° IDENTIFYING SUSPICIOUS SAMPLES")
print("="*60)

# 3.1 Score consistency check
df['score_std'] = df[['a_score', 'b_score', 'c_final_score']].std(axis=1)
high_variance = df[df['score_std'] > 30]  # High disagreement between pipeline scores
print(f"\nüî∏ High variance between pipeline scores (>30 std): {len(high_variance)} samples")

# 3.2 Text length anomalies
df['total_text_length'] = df['resume_length'] + df['jd_length']
text_length_stats = df['total_text_length'].describe()
q1 = text_length_stats['25%']
q3 = text_length_stats['75%']
iqr = q3 - q1
lower_bound = q1 - 1.5 * iqr
upper_bound = q3 + 1.5 * iqr

text_outliers = df[(df['total_text_length'] < lower_bound) | (df['total_text_length'] > upper_bound)]
print(f"üî∏ Text length outliers (IQR method): {len(text_outliers)} samples")

# 3.3 Aspect score anomalies (for Pipeline C)
if all(col in df.columns for col in ['c_skill', 'c_experience', 'c_domain', 'c_tools', 'c_seniority', 'c_education']):
    aspect_cols = ['c_skill', 'c_experience', 'c_domain', 'c_tools', 'c_seniority', 'c_education']
    df['aspect_mean'] = df[aspect_cols].mean(axis=1)
    df['aspect_std'] = df[aspect_cols].std(axis=1)

    # Check for unrealistic aspect scores (all 0 or all 100)
    extreme_aspects = df[(df[aspect_cols].min(axis=1) == 0) & (df[aspect_cols].max(axis=1) == 100)]
    print(f"üî∏ Extreme aspect scores (0s and 100s): {len(extreme_aspects)} samples")

# 3.4 Decision inconsistency patterns
def get_decision_pattern(row):
    decisions = [row['a_decision'][0], row['b_decision'][0], row['c_decision'][0]]
    return ''.join(decisions)

df['decision_pattern'] = df.apply(get_decision_pattern, axis=1)

# Identify rare decision patterns (potential anomalies)
pattern_counts = df['decision_pattern'].value_counts()
rare_patterns = pattern_counts[pattern_counts <= 2].index.tolist()
rare_pattern_samples = df[df['decision_pattern'].isin(rare_patterns)]
print(f"üî∏ Rare decision patterns (‚â§2 occurrences): {len(rare_pattern_samples)} samples")

from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler

# ============================================================
# 4. MACHINE LEARNING BASED ANOMALY DETECTION
# ============================================================
print("\n" + "="*60)
print("ü§ñ MACHINE LEARNING ANOMALY DETECTION")
print("="*60)

# Prepare features for anomaly detection
features_for_anomaly = []

# Numerical features
if 'a_score' in df.columns:
    features_for_anomaly.append('a_score')
if 'b_score' in df.columns:
    features_for_anomaly.append('b_score')
if 'c_final_score' in df.columns:
    features_for_anomaly.append('c_final_score')
if 'resume_length' in df.columns:
    features_for_anomaly.append('resume_length')
if 'jd_length' in df.columns:
    features_for_anomaly.append('jd_length')

# Add aspect scores if available
if all(col in df.columns for col in ['c_skill', 'c_experience', 'c_domain', 'c_tools', 'c_seniority', 'c_education']):
    features_for_anomaly.extend(['c_skill', 'c_experience', 'c_domain', 'c_tools', 'c_seniority', 'c_education'])

if len(features_for_anomaly) >= 3:  # Need enough features for anomaly detection
    print(f"\nüîç Using {len(features_for_anomaly)} features for anomaly detection:")
    print(features_for_anomaly)

    # Handle missing values
    X = df[features_for_anomaly].fillna(df[features_for_anomaly].median())

    # Standardize features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Use Isolation Forest for anomaly detection
    iso_forest = IsolationForest(contamination=0.1, random_state=42)  # Assume 10% anomalies
    anomalies = iso_forest.fit_predict(X_scaled)

    df['is_anomaly'] = (anomalies == -1).astype(int)
    anomalies_df = df[df['is_anomaly'] == 1]

    print(f"\nü§ñ Isolation Forest detected {len(anomalies_df)} anomalous samples")

    # Analyze characteristics of anomalies
    print("\nüìä Anomaly Characteristics:")
    for feature in features_for_anomaly[:5]:  # Show top 5 features
        anomaly_mean = anomalies_df[feature].mean()
        overall_mean = df[feature].mean()
        diff = anomaly_mean - overall_mean
        print(f"  {feature}: Anomalies avg={anomaly_mean:.1f}, Overall avg={overall_mean:.1f}, Diff={diff:.1f}")
else:
    print("‚ö†Ô∏è Not enough numerical features for anomaly detection")

# ============================================================
# 5. COMBINE ALL SUSPICIOUS SAMPLES
# ============================================================
print("\n" + "="*60)
print("üìä COMBINING ALL SUSPICIOUS SAMPLE DETECTIONS")
print("="*60)

# Create flags for different types of suspicious samples
suspicious_reasons = []

# Critical errors
if 'critical_error' in df.columns:
    df['flag_critical'] = df['critical_error']
    suspicious_reasons.append('critical_error')

# High score variance
if 'score_std' in df.columns:
    df['flag_high_variance'] = (df['score_std'] > 30).astype(int)
    suspicious_reasons.append('high_variance')

# Text outliers
if 'total_text_length' in df.columns:
    df['flag_text_outlier'] = ((df['total_text_length'] < lower_bound) |
                                (df['total_text_length'] > upper_bound)).astype(int)
    suspicious_reasons.append('text_outlier')

# ML anomalies
if 'is_anomaly' in df.columns:
    df['flag_ml_anomaly'] = df['is_anomaly']
    suspicious_reasons.append('ml_anomaly')

# Rare decision patterns
if 'decision_pattern' in df.columns:
    rare_patterns_mask = df['decision_pattern'].isin(rare_patterns) if len(rare_patterns) > 0 else pd.Series(False, index=df.index)
    df['flag_rare_pattern'] = rare_patterns_mask.astype(int)
    suspicious_reasons.append('rare_pattern')

# Count flags for each sample
if suspicious_reasons:
    flag_columns = [f'flag_{reason}' for reason in ['critical', 'high_variance', 'text_outlier', 'ml_anomaly', 'rare_pattern']
                   if f'flag_{reason}' in df.columns]

    df['suspicious_score'] = df[flag_columns].sum(axis=1)

    print("\nüìà Suspicious Samples Distribution:")
    score_dist = df['suspicious_score'].value_counts().sort_index()
    for score, count in score_dist.items():
        percentage = count / len(df) * 100
        print(f"  Score {score}: {count} samples ({percentage:.1f}%)")

    # Define threshold for removal
    removal_threshold = 2  # Remove samples with 2 or more flags
    to_remove = df[df['suspicious_score'] >= removal_threshold]
    to_keep = df[df['suspicious_score'] < removal_threshold]

    print(f"\nüóëÔ∏è  Recommended for removal (‚â•{removal_threshold} flags): {len(to_remove)} samples")
    print(f"‚úÖ To keep (<{removal_threshold} flags): {len(to_keep)} samples")

    # Analyze what we're removing
    print("\nüîç Analysis of samples to be removed:")
    if len(to_remove) > 0:
        print(f"  Average accuracy in to-remove set:")
        print(f"    Pipeline A: {to_remove['a_correct'].mean():.2%}")
        print(f"    Pipeline B: {to_remove['b_correct'].mean():.2%}")
        print(f"    Pipeline C: {to_remove['c_correct'].mean():.2%}")

        print(f"\n  Most common flags in to-remove set:")
        for flag in flag_columns:
            flag_count = to_remove[flag].sum()
            if flag_count > 0:
                print(f"    {flag}: {flag_count} samples")

# ============================================================
# 6. VISUALIZE SUSPICIOUS SAMPLES
# ============================================================
print("\n" + "="*60)
print("üìä VISUALIZING SUSPICIOUS SAMPLES")
print("="*60)

fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# 1. Score distributions by suspicious score
if 'suspicious_score' in df.columns:
    for score in sorted(df['suspicious_score'].unique()):
        subset = df[df['suspicious_score'] == score]
        axes[0,0].hist(subset['c_final_score'], alpha=0.5,
                      label=f'Score {score} (n={len(subset)})', bins=20, density=True)
    axes[0,0].set_xlabel('Pipeline C Score')
    axes[0,0].set_ylabel('Density')
    axes[0,0].set_title('Score Distribution by Suspicious Score')
    axes[0,0].legend()
    axes[0,0].grid(True, alpha=0.3)

# 2. Text length vs score
if 'total_text_length' in df.columns and 'c_final_score' in df.columns:
    scatter = axes[0,1].scatter(df['total_text_length'], df['c_final_score'],
                               c=df['suspicious_score'] if 'suspicious_score' in df.columns else 'blue',
                               alpha=0.6, cmap='viridis')
    axes[0,1].set_xlabel('Total Text Length (resume + JD)')
    axes[0,1].set_ylabel('Pipeline C Score')
    axes[0,1].set_title('Text Length vs Score')
    if 'suspicious_score' in df.columns:
        plt.colorbar(scatter, ax=axes[0,1], label='Suspicious Score')
    axes[0,1].grid(True, alpha=0.3)

# 3. Pipeline agreement vs accuracy
if 'all_agree' in df.columns and 'c_correct' in df.columns:
    agreement_groups = df.groupby('all_agree')['c_correct'].mean()
    axes[0,2].bar(['Disagree', 'Agree'], agreement_groups.values)
    axes[0,2].set_ylabel('Pipeline C Accuracy')
    axes[0,2].set_title('Pipeline Agreement vs Accuracy')
    axes[0,2].set_ylim(0, 1)
    axes[0,2].grid(True, alpha=0.3, axis='y')

    # Add value labels
    for i, v in enumerate(agreement_groups.values):
        axes[0,2].text(i, v + 0.02, f'{v:.2%}', ha='center')

# 4. Flag correlation heatmap
if len(flag_columns) > 1:
    flag_corr = df[flag_columns].corr()
    im = axes[1,0].imshow(flag_corr, cmap='coolwarm', vmin=-1, vmax=1)
    axes[1,0].set_xticks(range(len(flag_columns)))
    axes[1,0].set_xticklabels([col.replace('flag_', '') for col in flag_columns], rotation=45)
    axes[1,0].set_yticks(range(len(flag_columns)))
    axes[1,0].set_yticklabels([col.replace('flag_', '') for col in flag_columns])
    axes[1,0].set_title('Flag Correlation Heatmap')
    plt.colorbar(im, ax=axes[1,0])

# 5. Decision patterns distribution
if 'decision_pattern' in df.columns:
    top_patterns = df['decision_pattern'].value_counts().head(10)
    axes[1,1].bar(range(len(top_patterns)), top_patterns.values)
    axes[1,1].set_xticks(range(len(top_patterns)))
    axes[1,1].set_xticklabels(top_patterns.index, rotation=45)
    axes[1,1].set_xlabel('Decision Pattern (A,B,C where G=GoodFit, N=NoFit)')
    axes[1,1].set_ylabel('Count')
    axes[1,1].set_title('Top 10 Decision Patterns')
    axes[1,1].grid(True, alpha=0.3, axis='y')

# 6. Suspicious score distribution
if 'suspicious_score' in df.columns:
    score_counts = df['suspicious_score'].value_counts().sort_index()
    bars = axes[1,2].bar(score_counts.index, score_counts.values)
    axes[1,2].set_xlabel('Suspicious Score')
    axes[1,2].set_ylabel('Number of Samples')
    axes[1,2].set_title('Distribution of Suspicious Scores')
    axes[1,2].grid(True, alpha=0.3, axis='y')

    # Color bars by score
    for bar, score in zip(bars, score_counts.index):
        if score >= removal_threshold:
            bar.set_color('red')
        elif score > 0:
            bar.set_color('orange')

    # Add value labels
    for i, v in enumerate(score_counts.values):
        axes[1,2].text(score_counts.index[i], v + 0.5, str(v), ha='center')

plt.tight_layout()
plt.show()

"""### Save Dropped Scritical Sample"""

# ============================================================
# 7. CREATE CLEANED DATASET
# ============================================================
print("\n" + "="*60)
print("üßπ CREATING CLEANED DATASET")
print("="*60)

if 'suspicious_score' in df.columns:
    # Option 1: Remove highly suspicious samples
    cleaned_df = df[df['suspicious_score'] < removal_threshold].copy()

    # Option 2: Create a "review needed" dataset
    review_needed_df = df[df['suspicious_score'] >= removal_threshold].copy()

    print(f"\nüìä Dataset Split:")
    print(f"  Clean dataset: {len(cleaned_df)} samples ({len(cleaned_df)/len(df):.1%})")
    print(f"  Review needed: {len(review_needed_df)} samples ({len(review_needed_df)/len(df):.1%})")

    # Compare performance on cleaned vs original
    print("\nüìà Performance Comparison:")
    metrics = ['a_correct', 'b_correct', 'c_correct']
    metric_names = ['Pipeline A', 'Pipeline B', 'Pipeline C']

    comparison_results = []
    for metric, name in zip(metrics, metric_names):
        if metric in df.columns:
            orig_acc = df[metric].mean()
            clean_acc = cleaned_df[metric].mean() if len(cleaned_df) > 0 else 0
            diff = clean_acc - orig_acc

            comparison_results.append({
                'Pipeline': name,
                'Original_Accuracy': orig_acc,
                'Cleaned_Accuracy': clean_acc,
                'Difference': diff
            })

    comparison_df = pd.DataFrame(comparison_results)
    print(comparison_df.round(4))

    # Save cleaned dataset
    cleaned_df.to_csv(CLEANED_DIR, index=False)
    review_needed_df.to_csv('needs_review_pipeline_results.csv', index=False)

    print("\nüíæ Files saved:")
    print(f"  ‚úÖ Cleaned dataset: {CLEANED_DIR}")
    print(f"  ‚ö†Ô∏è  Review needed: needs_review_pipeline_results.csv")

# ============================================================
# 8. RE-OPTIMIZE PIPELINE C ON CLEANED DATA
# ============================================================
print("\n" + "="*60)
print("üîÑ RE-OPTIMIZING PIPELINE C ON CLEANED DATA")
print("="*60)

if len(cleaned_df) > 10:  # Need enough samples for optimization
    from sklearn.metrics import roc_curve, auc

    # Threshold optimization on cleaned data
    def find_optimal_threshold_cleaned(scores, true_labels):
        y_true_binary = (true_labels == 'Good Fit').astype(int)
        fpr, tpr, thresholds = roc_curve(y_true_binary, scores)

        # Find optimal threshold (Youden's J)
        youden_j = tpr - fpr
        optimal_idx = np.argmax(youden_j)
        optimal_threshold = thresholds[optimal_idx]

        return optimal_threshold

    optimal_threshold_clean = find_optimal_threshold_cleaned(
        cleaned_df['c_final_score'],
        cleaned_df['true_label']
    )

    print(f"\nüéØ Threshold Optimization on Cleaned Data:")
    print(f"  Original threshold: {THRESHOLD}")
    print(f"  Optimal threshold (cleaned data): {optimal_threshold_clean:.1f}")

    # Weight optimization suggestion
    if all(col in cleaned_df.columns for col in ['c_skill', 'c_experience', 'c_domain',
                                                 'c_tools', 'c_seniority', 'c_education']):
        aspect_cols = ['c_skill', 'c_experience', 'c_domain', 'c_tools', 'c_seniority', 'c_education']

        # Calculate correlation with correctness on cleaned data
        aspect_correlations_clean = {}
        for col in aspect_cols:
            corr = cleaned_df[col].corr(cleaned_df['c_correct'])
            aspect_correlations_clean[col] = corr

        print("\n‚öñÔ∏è Aspect Correlation with Correctness (Cleaned Data):")
        for aspect, corr in aspect_correlations_clean.items():
            aspect_name = aspect.replace('c_', '').title()
            print(f"  {aspect_name}: {corr:.3f}")

        # Suggest weight adjustments based on correlation
        print("\nüí° Suggested Weight Adjustments:")
        print("  (Increase weights for aspects with positive correlation)")
        print("  (Decrease weights for aspects with negative correlation)")

        # Current weights
        current_weights = {
            'c_skill': 0.35,
            'c_experience': 0.25,
            'c_domain': 0.10,
            'c_tools': 0.15,
            'c_seniority': 0.05,
            'c_education': 0.10
        }

        # Simple heuristic: adjust proportionally to correlation
        total_corr = sum(abs(corr) for corr in aspect_correlations_clean.values())
        if total_corr > 0:
            suggested_weights = {}
            for aspect in aspect_cols:
                # Weight proportional to absolute correlation
                suggested_weights[aspect] = abs(aspect_correlations_clean[aspect]) / total_corr

            # Normalize to sum to 1
            weight_sum = sum(suggested_weights.values())
            suggested_weights = {k: v/weight_sum for k, v in suggested_weights.items()}

            print("\nüìä Suggested Weights vs Current Weights:")
            for aspect in aspect_cols:
                aspect_name = aspect.replace('c_', '').title()
                current = current_weights.get(aspect, 0)
                suggested = suggested_weights.get(aspect, 0)
                change = suggested - current

                print(f"  {aspect_name}:")
                print(f"    Current: {current:.3f}")
                print(f"    Suggested: {suggested:.3f}")
                print(f"    Change: {change:+.3f} ({'‚Üë' if change > 0 else '‚Üì'})")

"""## Update Result Validation With Weights and Threshold"""

# ============================================================
# RESULT UPDATE WITH WEIGHTS AND THRESHOLD
# ============================================================

print(f"üéØ Updating {RESULTS_DIR} in place...")
print(f"üìä Configuration: threshold={THRESHOLD}, weights={WEIGHTS}")

# Load data
df = pd.read_csv(RESULTS_DIR)

# Backup original values
if 'c_final_score_original' not in df.columns:
    df['c_final_score_original'] = df['c_final_score']
if 'c_decision_original' not in df.columns:
    df['c_decision_original'] = df['c_decision']

# Calculate new weighted score
def calculate_weighted_score(row):
    score = 0
    for aspect, weight in WEIGHTS.items():
        if aspect in row:
            score += row[aspect] * weight
    return round(score)

df['c_final_score'] = df.apply(calculate_weighted_score, axis=1)
df['c_final_score'] = df['c_final_score'].clip(0, 100)

# Update decision
df['c_decision'] = df['c_final_score'].apply(
    lambda x: 'Good Fit' if x >= THRESHOLD else 'No Fit'
)

# Save back to same file
df.to_csv(RESULTS_DIR, index=False)

# Show changes
if 'true_label' in df.columns:
    new_accuracy = (df['c_decision'] == df['true_label']).mean()
    old_accuracy = (df['c_decision_original'] == df['true_label']).mean()

    print(f"\nüìà Accuracy comparison:")
    print(f"  Original: {old_accuracy:.2%}")
    print(f"  Updated:  {new_accuracy:.2%}")
    print(f"  Change:   {new_accuracy - old_accuracy:+.2%}")

print(f"\nüìä Decision distribution:")
print(f"  Good Fit: {(df['c_decision'] == 'Good Fit').sum()} samples")
print(f"  No Fit:   {(df['c_decision'] == 'No Fit').sum()} samples")

print(f"\n‚úÖ File updated successfully: {RESULTS_DIR}")
print("üíæ Original values saved in columns: c_final_score_original, c_decision_original")

"""## Process 3: Optimized Weights & Threshold"""

# ============================================================
# 3. PIPELINE C THRESHOLD OPTIMIZATION
# ============================================================
print("\n" + "="*60)
print("üéöÔ∏è PIPELINE C THRESHOLD OPTIMIZATION")
print("="*60)

def evaluate_threshold(threshold, scores, true_labels):
    """Evaluate performance at a given threshold"""
    predictions = (scores >= threshold).astype(int)
    true_binary = (true_labels == 'Good Fit').astype(int)

    # Calculate metrics
    tp = ((true_binary == 1) & (predictions == 1)).sum()
    fp = ((true_binary == 0) & (predictions == 1)).sum()
    tn = ((true_binary == 0) & (predictions == 0)).sum()
    fn = ((true_binary == 1) & (predictions == 0)).sum()

    accuracy = (tp + tn) / len(scores)
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

    return {
        'threshold': threshold,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'tp': tp,
        'fp': fp,
        'tn': tn,
        'fn': fn
    }

# Test different thresholds
thresholds = np.arange(30, 90, 2)  # Test from 30 to 88
results = []

for thresh in thresholds:
    results.append(evaluate_threshold(thresh, df['c_final_score'], df['true_label']))

threshold_df = pd.DataFrame(results)

# Find optimal thresholds by different metrics
optimal_f1 = threshold_df.loc[threshold_df['f1'].idxmax()]
optimal_accuracy = threshold_df.loc[threshold_df['accuracy'].idxmax()]
optimal_youden = threshold_df.loc[(threshold_df['recall'] - (1 - threshold_df['accuracy'])).idxmax()]

print(f"\nüìà Current threshold: {THRESHOLD}")
print(f"üìä Current performance at threshold {THRESHOLD}:")
current_perf = threshold_df[threshold_df['threshold'] == THRESHOLD]
if len(current_perf) > 0:
    current = current_perf.iloc[0]
    print(f"  Accuracy: {current['accuracy']:.2%}")
    print(f"  F1-Score: {current['f1']:.2%}")

print(f"\nüéØ Optimal threshold by F1-Score: {optimal_f1['threshold']}")
print(f"  Accuracy: {optimal_f1['accuracy']:.2%}")
print(f"  F1-Score: {optimal_f1['f1']:.2%}")
print(f"  Precision: {optimal_f1['precision']:.2%}")
print(f"  Recall: {optimal_f1['recall']:.2%}")

print(f"\nüéØ Optimal threshold by Accuracy: {optimal_accuracy['threshold']}")
print(f"  Accuracy: {optimal_accuracy['accuracy']:.2%}")

# Visualization
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Threshold vs Metrics
axes[0,0].plot(threshold_df['threshold'], threshold_df['accuracy'], 'b-', label='Accuracy', linewidth=2)
axes[0,0].plot(threshold_df['threshold'], threshold_df['f1'], 'g-', label='F1-Score', linewidth=2)
axes[0,0].axvline(THRESHOLD, color='r', linestyle='--', alpha=0.7, label='Current (THRESHOLD)')
axes[0,0].axvline(optimal_f1['threshold'], color='orange', linestyle='--', alpha=0.7, label=f'Optimal F1 ({optimal_f1["threshold"]})')
axes[0,0].set_xlabel('Threshold')
axes[0,0].set_ylabel('Score')
axes[0,0].set_title('Pipeline C: Performance vs Threshold')
axes[0,0].legend()
axes[0,0].grid(True, alpha=0.3)

# Precision-Recall tradeoff
axes[0,1].plot(threshold_df['recall'], threshold_df['precision'], 'b-', linewidth=2)
axes[0,1].scatter(optimal_f1['recall'], optimal_f1['precision'], color='red', s=100,
                  label=f'Optimal F1 (thresh={optimal_f1["threshold"]})')
axes[0,1].set_xlabel('Recall')
axes[0,1].set_ylabel('Precision')
axes[0,1].set_title('Precision-Recall Curve')
axes[0,1].legend()
axes[0,1].grid(True, alpha=0.3)

# Distribution of scores by true label
axes[1,0].hist(df[df['true_label'] == 'Good Fit']['c_final_score'],
               alpha=0.5, label='Good Fit', bins=20, density=True)
axes[1,0].hist(df[df['true_label'] == 'No Fit']['c_final_score'],
               alpha=0.5, label='No Fit', bins=20, density=True)
axes[1,0].axvline(THRESHOLD, color='r', linestyle='--', alpha=0.7, label='Current (THRESHOLD)')
axes[1,0].axvline(optimal_f1['threshold'], color='orange', linestyle='--', alpha=0.7,
                  label=f'Optimal ({optimal_f1["threshold"]})')
axes[1,0].set_xlabel('Pipeline C Score')
axes[1,0].set_ylabel('Density')
axes[1,0].set_title('Score Distribution by True Label')
axes[1,0].legend()
axes[1,0].grid(True, alpha=0.3)

# Error rates vs threshold
false_pos_rate = threshold_df['fp'] / (threshold_df['fp'] + threshold_df['tn'])
false_neg_rate = threshold_df['fn'] / (threshold_df['fn'] + threshold_df['tp'])

axes[1,1].plot(threshold_df['threshold'], false_pos_rate, 'r-', label='False Positive Rate', linewidth=2)
axes[1,1].plot(threshold_df['threshold'], false_neg_rate, 'b-', label='False Negative Rate', linewidth=2)
axes[1,1].axvline(THRESHOLD, color='black', linestyle='--', alpha=0.7, label='Current (THRESHOLD)')
axes[1,1].axvline(optimal_f1['threshold'], color='orange', linestyle='--', alpha=0.7,
                  label=f'Optimal ({optimal_f1["threshold"]})')
axes[1,1].set_xlabel('Threshold')
axes[1,1].set_ylabel('Error Rate')
axes[1,1].set_title('Error Rates vs Threshold')
axes[1,1].legend()
axes[1,1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ============================================================
# 9. THRESHOLD ANALYSIS (FOR SCORE-BASED PIPELINES)
# ============================================================
print("\nüéöÔ∏è Threshold Analysis")

def find_optimal_threshold(scores, true_labels, pos_label='Good Fit'):
    """Find optimal threshold for binary classification"""
    from sklearn.metrics import roc_curve, auc

    # Convert to binary
    y_true_binary = (true_labels == pos_label).astype(int)

    # Calculate ROC curve
    fpr, tpr, thresholds = roc_curve(y_true_binary, scores)
    roc_auc = auc(fpr, tpr)

    # Find optimal threshold (Youden's J statistic)
    youden_j = tpr - fpr
    optimal_idx = np.argmax(youden_j)
    optimal_threshold = thresholds[optimal_idx]

    return optimal_threshold, roc_auc, fpr, tpr

# Analyze Pipeline C threshold
if 'c_final_score' in df_clean.columns:
    optimal_threshold, roc_auc, fpr, tpr = find_optimal_threshold(
        df_clean['c_final_score'],
        df_clean['true_label']
    )

    print(f"\nüìà Pipeline C ROC Analysis:")
    print(f"  Current threshold: {THRESHOLD}")
    print(f"  Optimal threshold: {optimal_threshold:.2f}")
    print(f"  ROC AUC: {roc_auc:.3f}")

    # Plot ROC curve
    fig8, ax8 = plt.subplots(figsize=(10, 8))
    ax8.plot(fpr, tpr, 'b-', label=f'ROC curve (AUC = {roc_auc:.3f})')
    ax8.plot([0, 1], [0, 1], 'r--', label='Random')
    ax8.scatter(fpr[np.argmax(tpr - fpr)], tpr[np.argmax(tpr - fpr)],
                color='red', s=100, label=f'Optimal threshold: {optimal_threshold:.2f}')
    ax8.set_xlabel('False Positive Rate')
    ax8.set_ylabel('True Positive Rate')
    ax8.set_title('ROC Curve - Pipeline C')
    ax8.legend()
    ax8.grid(True, alpha=0.3)
    plt.show()

# ============================================================
# 4. WEIGHT OPTIMIZATION FOR ASPECT SCORES
# ============================================================
print("\n" + "="*60)
print("‚öñÔ∏è PIPELINE C WEIGHT OPTIMIZATION")
print("="*60)

# Check if aspect scores exist
aspect_cols = ['c_skill', 'c_experience', 'c_domain', 'c_tools', 'c_seniority', 'c_education']
if all(col in df_clean.columns for col in aspect_cols):


    print("\nüî¢ Current Weights:")
    for aspect, weight in WEIGHTS.items():
        print(f"  {aspect.replace('c_', '').title()}: {weight:.2f}")

    # 1. Correlation with correctness
    print("\nüìä Correlation of Aspect Scores with Pipeline C Correctness:")
    aspect_correlations = {}
    for col in aspect_cols:
        corr = df_clean[col].corr(df_clean['c_correct'])
        aspect_correlations[col] = corr
        print(f"  {col.replace('c_', '').title()}: {corr:.3f}")

    # 2. Aspect scores distribution by correctness
    print("\nüìà Aspect Scores by Correctness:")
    aspect_stats = []
    for col in aspect_cols:
        correct_mean = df_clean[df_clean['c_correct'] == 1][col].mean()
        incorrect_mean = df_clean[df_clean['c_correct'] == 0][col].mean()
        diff = correct_mean - incorrect_mean

        aspect_stats.append({
            'Aspect': col.replace('c_', ''),
            'Correct_Mean': correct_mean,
            'Incorrect_Mean': incorrect_mean,
            'Difference': diff,
            'Abs_Difference': abs(diff)
        })

    aspect_stats_df = pd.DataFrame(aspect_stats)
    print(aspect_stats_df.round(2))

    # 3. Optimize weights based on correlation
    def calculate_weighted_score(row, weights):
        """Calculate weighted score with given weights"""
        total = 0
        for aspect, weight in weights.items():
            total += row[aspect] * weight
        return total

    def optimize_weights_random_search(df, n_iterations=1000):
        """Random search for optimal weights"""
        best_accuracy = 0
        best_weights = None
        best_threshold = THRESHOLD

        for _ in range(n_iterations):
            # Generate random weights (sum to 1)
            weights = np.random.dirichlet(np.ones(len(aspect_cols)))
            weight_dict = dict(zip(aspect_cols, weights))

            # Calculate weighted scores
            weighted_scores = df_clean.apply(lambda row: calculate_weighted_score(row, weight_dict), axis=1)

            # Try different thresholds
            for threshold in range(40, 80, 5):
                predictions = (weighted_scores >= threshold).astype(int)
                true_binary = (df_clean['true_label'] == 'Good Fit').astype(int)
                accuracy = (predictions == true_binary).mean()

                if accuracy > best_accuracy:
                    best_accuracy = accuracy
                    best_weights = weight_dict.copy()
                    best_threshold = threshold

        return best_weights, best_threshold, best_accuracy

    # Run optimization
    print("\nüîÑ Optimizing weights (random search)...")
    optimized_weights, optimized_threshold, optimized_accuracy = optimize_weights_random_search(df, n_iterations=500)

    print(f"\nüéØ Optimized Weights Found:")
    for aspect, weight in optimized_weights.items():
        aspect_name = aspect.replace('c_', '').title()
        print(f"  {aspect_name}: {weight:.3f}")

    print(f"\nüìä Performance with Optimized Weights:")
    print(f"  Optimal Threshold: {optimized_threshold}")
    print(f"  Accuracy: {optimized_accuracy:.2%}")

    # Compare with current
    current_weighted = df_clean.apply(lambda row: calculate_weighted_score(row, WEIGHTS), axis=1)
    current_predictions = (current_weighted >= THRESHOLD).astype(int)
    current_true = (df_clean['true_label'] == 'Good Fit').astype(int)
    current_accuracy = (current_predictions == current_true).mean()

    print(f"\nüìà Improvement: {optimized_accuracy - current_accuracy:.2%} points")

    # Visualization of weight optimization
    fig2, axes2 = plt.subplots(2, 2, figsize=(15, 10))

    # Current vs Optimized weights
    current_weights_df = pd.DataFrame({
        'Aspect': [k.replace('c_', '').title() for k in WEIGHTS.keys()],
        'Weight': list(WEIGHTS.values()),
        'Type': 'Current'
    })

    optimized_weights_df = pd.DataFrame({
        'Aspect': [k.replace('c_', '').title() for k in optimized_weights.keys()],
        'Weight': list(optimized_weights.values()),
        'Type': 'Optimized'
    })

    weights_comparison = pd.concat([current_weights_df, optimized_weights_df])

    sns.barplot(data=weights_comparison, x='Aspect', y='Weight', hue='Type', ax=axes2[0,0])
    axes2[0,0].set_title('Current vs Optimized Weights')
    axes2[0,0].set_ylabel('Weight')
    axes2[0,0].tick_params(axis='x', rotation=45)

    # Aspect importance (based on correlation)
    importance_df = pd.DataFrame({
        'Aspect': [k.replace('c_', '').title() for k in aspect_correlations.keys()],
        'Correlation': list(aspect_correlations.values()),
        'Abs_Correlation': [abs(v) for v in aspect_correlations.values()]
    }).sort_values('Abs_Correlation', ascending=False)

    sns.barplot(data=importance_df, x='Correlation', y='Aspect', ax=axes2[0,1])
    axes2[0,1].axvline(0, color='black', linewidth=0.5)
    axes2[0,1].set_title('Aspect Correlation with Correctness')
    axes2[0,1].set_xlabel('Correlation')

    # Score distributions with different weights
    axes2[1,0].hist(current_weighted, alpha=0.5, label='Current Weights', bins=20, density=True)
    optimized_weighted = df_clean.apply(lambda row: calculate_weighted_score(row, optimized_weights), axis=1)
    axes2[1,0].hist(optimized_weighted, alpha=0.5, label='Optimized Weights', bins=20, density=True)
    axes2[1,0].axvline(THRESHOLD, color='r', linestyle='--', alpha=0.7, label='Current Threshold (THRESHOLD)')
    axes2[1,0].axvline(optimized_threshold, color='orange', linestyle='--', alpha=0.7,
                       label=f'Optimized Threshold ({optimized_threshold})')
    axes2[1,0].set_xlabel('Weighted Score')
    axes2[1,0].set_ylabel('Density')
    axes2[1,0].set_title('Score Distributions with Different Weights')
    axes2[1,0].legend()
    axes2[1,0].grid(True, alpha=0.3)

    # Performance comparison
    comparison_data = pd.DataFrame({
        'Configuration': ['Current (thresh=THRESHOLD)', 'Optimized'],
        'Accuracy': [current_accuracy, optimized_accuracy]
    })

    sns.barplot(data=comparison_data, x='Configuration', y='Accuracy', ax=axes2[1,1])
    axes2[1,1].set_ylim(0, 1)
    axes2[1,1].set_title('Performance Comparison')
    axes2[1,1].set_ylabel('Accuracy')

    # Add value labels
    for i, v in enumerate(comparison_data['Accuracy']):
        axes2[1,1].text(i, v + 0.02, f'{v:.2%}', ha='center')

    plt.tight_layout()
    plt.show()

    # 4. Aspect contribution analysis
    print("\n" + "="*60)
    print("üìä ASPECT CONTRIBUTION ANALYSIS")
    print("="*60)

    def analyze_aspect_contribution(df, weights):
        """Analyze how each aspect contributes to final decision"""
        contributions = {}

        for aspect in aspect_cols:
            # Calculate contribution for each sample
            aspect_contributions = df_clean[aspect] * weights[aspect]
            contributions[aspect] = aspect_contributions

        contribution_df = pd.DataFrame(contributions)
        contribution_df['total'] = contribution_df.sum(axis=1)

        # Normalize contributions
        for aspect in aspect_cols:
            contribution_df[f'{aspect}_pct'] = contribution_df[aspect] / contribution_df['total'] * 100

        return contribution_df

    current_contributions = analyze_aspect_contribution(df, WEIGHTS)
    optimized_contributions = analyze_aspect_contribution(df, optimized_weights)

    print("\nüìà Average Aspect Contributions (% of total score):")
    contribution_summary = pd.DataFrame({
        'Aspect': [col.replace('c_', '').title() for col in aspect_cols],
        'Current_Avg_Pct': [current_contributions[f'{col}_pct'].mean() for col in aspect_cols],
        'Optimized_Avg_Pct': [optimized_contributions[f'{col}_pct'].mean() for col in aspect_cols]
    })

    print(contribution_summary.round(1))

else:
    print("‚ùå Aspect score columns not found. Skipping weight optimization.")

# ============================================================
# 6. RECOMMENDATIONS FOR PIPELINE C IMPROVEMENT
# ============================================================
print("\n" + "="*60)
print("üí° RECOMMENDATIONS FOR PIPELINE C IMPROVEMENT")
print("="*60)

print("\nüéØ 1. THRESHOLD ADJUSTMENT:")
print(f"   ‚Ä¢ Current threshold: {THRESHOLD}")
print(f"   ‚Ä¢ Recommended threshold: {optimized_threshold}")
print(f"   ‚Ä¢ Expected accuracy improvement: {optimized_accuracy - current_accuracy:.2%}")

if 'optimized_weights' in locals():
    print("\n‚öñÔ∏è 2. WEIGHT OPTIMIZATION:")
    print("   ‚Ä¢ Current weights may not be optimal")
    print("   ‚Ä¢ Recommended weights based on data correlation:")
    for aspect, weight in optimized_weights.items():
        aspect_name = aspect.replace('c_', '').title()
        current_weight = WEIGHTS.get(aspect, 0)
        change = weight - current_weight
        print(f"     {aspect_name}: {current_weight:.2f} ‚Üí {weight:.3f} ({change:+.3f})")

    print(f"\n   ‚Ä¢ Expected accuracy with optimized weights: {optimized_accuracy:.2%}")

print("\nüîç 3. ERROR ANALYSIS INSIGHTS:")
if len(all_wrong_samples) > 0:
    print(f"   ‚Ä¢ Found {len(all_wrong_samples)} samples where all pipelines fail")
    print("   ‚Ä¢ Common characteristics of these samples:")

    # Identify patterns
    if 'all_good_fit' in all_wrong_samples.columns:
        false_positives = all_wrong_samples['all_good_fit'].sum()
        false_negatives = all_wrong_samples['all_no_fit'].sum()

        if false_positives > false_negatives:
            print("     - Majority are false positives (predict Good Fit but should be No Fit)")
            print("     - Consider increasing threshold or adjusting weights")
        else:
            print("     - Majority are false negatives (predict No Fit but should be Good Fit)")
            print("     - Consider decreasing threshold or adjusting weights")

print("\nüìä 4. ASPECT-SPECIFIC IMPROVEMENTS:")
if 'aspect_correlations' in locals():
    # Sort aspects by absolute correlation
    sorted_aspects = sorted(aspect_correlations.items(), key=lambda x: abs(x[1]), reverse=True)

    print("   ‚Ä¢ Focus on improving these aspects (highest correlation with correctness):")
    for aspect, corr in sorted_aspects[:3]:  # Top 3
        aspect_name = aspect.replace('c_', '').title()
        print(f"     {aspect_name}: correlation = {corr:.3f}")

        # Give specific advice based on correlation sign
        if corr > 0:
            print(f"       ‚Üí Higher scores in {aspect_name} correlate with correct predictions")
            print(f"       ‚Üí Consider increasing weight for this aspect")
        else:
            print(f"       ‚Üí Lower scores in {aspect_name} correlate with correct predictions")
            print(f"       ‚Üí This aspect might be overvalued")

"""### Save Optimized Config"""

# ============================================================
# 7. EXPORT OPTIMIZED CONFIGURATION
# ============================================================
print("\n" + "="*60)
print("üíæ EXPORTING OPTIMIZED CONFIGURATION")
print("="*60)

# Create configuration dictionary
config = {
    'current_config': {
        'threshold': THRESHOLD,
        'weights': WEIGHTS if 'WEIGHTS' in locals() else {},
        'accuracy': current_accuracy if 'current_accuracy' in locals() else None
    }
}

if 'optimized_weights' in locals():
    config['optimized_config'] = {
        'threshold': int(optimized_threshold),
        'weights': {k: float(v) for k, v in optimized_weights.items()},
        'expected_accuracy': float(optimized_accuracy)
    }

# Save to JSON
import json
with open(CONFIG_DIR, 'w') as f:
    json.dump(config, f, indent=2)

print(f"‚úÖ Saved optimized configuration to: {CONFIG_DIR}")

# Save detailed analysis
analysis_report = f"""
Pipeline C Optimization Analysis Report
========================================

Dataset Statistics:
-------------------
Total samples: {len(df)}
Pipeline C current accuracy: {df['c_correct'].mean():.2%}

Threshold Analysis:
------------------
Current threshold: {THRESHOLD}
Optimal threshold by F1-Score: {optimal_f1['threshold'] if 'optimal_f1' in locals() else 'N/A'}
Accuracy improvement: {(optimal_f1['accuracy'] - current_accuracy) if 'optimal_f1' in locals() and 'current' in locals() else 0:.2%}

Weight Optimization:
-------------------
Current weights: {WEIGHTS if 'WEIGHTS' in locals() else 'N/A'}
Optimized weights: {optimized_weights if 'optimized_weights' in locals() else 'N/A'}
Expected accuracy with optimized weights: {optimized_accuracy if 'optimized_accuracy' in locals() else 'N/A':.2%}

Error Analysis:
---------------
All pipelines wrong: {len(all_wrong_samples) if 'all_wrong_samples' in locals() else 0} samples
Only Pipeline C wrong: {len(only_c_wrong_samples) if 'only_c_wrong_samples' in locals() else 0} samples

Recommendations:
----------------
1. Adjust threshold to {optimal_f1['threshold'] if 'optimal_f1' in locals() else THRESHOLD}
2. Update weights as shown in optimized_config
3. Focus on improving aspect scoring for samples with high error rates
"""

with open('pipeline_c_analysis_report.txt', 'w') as f:
    f.write(analysis_report)

print("‚úÖ Saved detailed analysis report to: pipeline_c_analysis_report.txt")

# ============================================================
# 1. LOAD DATA AND CONFIGURATIONS
# ============================================================
# Load optimized configuration
print("\n‚öôÔ∏è Loading optimized configuration...")
try:
    with open(CONFIG_DIR, 'r') as f:
        config = json.load(f)

    print("‚úÖ Optimized configuration loaded")

    # Extract configurations
    if 'optimized_config' in config:
        opt_config = config['optimized_config']
        threshold_opt = opt_config.get('threshold', THRESHOLD)
        weights_opt = opt_config.get('weights', {})

        print(f"  Optimized threshold: {threshold_opt}")
        print(f"  Optimized weights: {weights_opt}")
    else:
        print("‚ö†Ô∏è No optimized config found, using default")
        threshold_opt = THRESHOLD
        weights_opt = {
            'c_skill': 0.30, 'c_experience': 0.30, 'c_domain': 0.15,
            'c_tools': 0.10, 'c_seniority': 0.10, 'c_education': 0.05
        }

except FileNotFoundError:
    print("‚ö†Ô∏è No optimized config found, using defaults")
    threshold_opt = THRESHOLD
    weights_opt = {
        'c_skill': 0.30, 'c_experience': 0.20, 'c_domain': 0.15,
        'c_tools': 0.10, 'c_seniority': 0.10, 'c_education': 0.05
    }
    config = {'current_config': {'threshold': THRESHOLD}}

"""## Experiment with new config"""

from sklearn.model_selection import train_test_split

# ============================================================
# 2. SPLIT DATA FOR VALIDATION
# ============================================================

print("\n" + "="*60)
print("üìä DATA SPLITTING FOR VALIDATION")
print("="*60)

# Ensure we have required columns
required_cols = ['true_label', 'c_final_score']
aspect_cols = ['c_skill', 'c_experience', 'c_domain', 'c_tools', 'c_seniority', 'c_education']

# Check if we have aspect scores
has_aspect_scores = all(col in df_clean.columns for col in aspect_cols)

# Prepare features and target
X = df_clean.copy()
y = (df_clean['true_label'] == 'Good Fit').astype(int)

# Split into train (for analysis) and test (for final validation)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

print(f"\nüìà Data Split:")
print(f"  Training set: {len(X_train)} samples ({len(X_train)/len(X):.1%})")
print(f"  Test set: {len(X_test)} samples ({len(X_test)/len(X):.1%})")

print(f"\nüìä Label Distribution in Test Set:")
print(f"  Good Fit: {y_test.sum()} samples ({(y_test.sum()/len(y_test)):.1%})")
print(f"  No Fit: {(len(y_test) - y_test.sum())} samples ({((len(y_test) - y_test.sum())/len(y_test)):.1%})")

# ============================================================
# 3. IMPLEMENT OPTIMIZED PIPELINE C
# ============================================================
print("\n" + "="*60)
print("‚öôÔ∏è IMPLEMENTING OPTIMIZED PIPELINE C")
print("="*60)

def calculate_weighted_score(row, weights):
    """Calculate weighted score with given weights"""
    total = 0
    for aspect, weight in weights.items():
        if aspect in row:
            total += row[aspect] * weight
    return total

def predict_pipeline_c(row, threshold, weights):
    """Make prediction using optimized Pipeline C"""
    # Calculate weighted score if weights provided
    if weights and has_aspect_scores:
        score = calculate_weighted_score(row, weights)
    else:
        score = row['c_final_score']

    # Apply threshold
    decision = 'Good Fit' if score >= threshold else 'No Fit'

    return decision, score

# Apply optimized pipeline to test set
print("\nüîß Applying optimized Pipeline C to test set...")

if has_aspect_scores:
    # Use weighted scores
    X_test['optimized_score'] = X_test.apply(
        lambda row: calculate_weighted_score(row, weights_opt),
        axis=1
    )
else:
    # Use original scores
    X_test['optimized_score'] = X_test['c_final_score']

# Make predictions
X_test['optimized_decision'] = X_test['optimized_score'].apply(
    lambda x: 'Good Fit' if x >= threshold_opt else 'No Fit'
)

# Calculate accuracy
optimized_correct = (X_test['optimized_decision'] == X_test['true_label']).mean()
original_correct = (X_test['c_decision'] == X_test['true_label']).mean()

print(f"\nüìä Performance Comparison (Test Set):")
print(f"  Original Pipeline C: {original_correct:.2%}")
print(f"  Optimized Pipeline C: {optimized_correct:.2%}")
print(f"  Improvement: {optimized_correct - original_correct:+.2%}")

# ============================================================
# 4. DETAILED PERFORMANCE VALIDATION
# ============================================================
print("\n" + "="*60)
print("üìà DETAILED PERFORMANCE VALIDATION")
print("="*60)

# Detailed classification reports
print("\nüìã Classification Report - Original Pipeline C:")
print(classification_report(
    X_test['true_label'],
    X_test['c_decision'],
    target_names=['No Fit', 'Good Fit']
))

print("\nüìã Classification Report - Optimized Pipeline C:")
print(classification_report(
    X_test['true_label'],
    X_test['optimized_decision'],
    target_names=['No Fit', 'Good Fit']
))

# Confusion matrices comparison
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Original confusion matrix
cm_original = confusion_matrix(X_test['true_label'], X_test['c_decision'],
                               labels=['No Fit', 'Good Fit'])
sns.heatmap(cm_original, annot=True, fmt='d', cmap='Blues',
            xticklabels=['No Fit', 'Good Fit'],
            yticklabels=['No Fit', 'Good Fit'],
            ax=axes[0])
axes[0].set_title('Original Pipeline C')
axes[0].set_xlabel('Predicted')
axes[0].set_ylabel('Actual')

# Optimized confusion matrix
cm_optimized = confusion_matrix(X_test['true_label'], X_test['optimized_decision'],
                                labels=['No Fit', 'Good Fit'])
sns.heatmap(cm_optimized, annot=True, fmt='d', cmap='Greens',
            xticklabels=['No Fit', 'Good Fit'],
            yticklabels=['No Fit', 'Good Fit'],
            ax=axes[1])
axes[1].set_title('Optimized Pipeline C')
axes[1].set_xlabel('Predicted')
axes[1].set_ylabel('Actual')

plt.tight_layout()
plt.show()

# Error analysis
print("\nüîç Error Analysis:")
errors_original = X_test[X_test['c_decision'] != X_test['true_label']]
errors_optimized = X_test[X_test['optimized_decision'] != X_test['true_label']]

print(f"  Original errors: {len(errors_original)} ({len(errors_original)/len(X_test):.1%})")
print(f"  Optimized errors: {len(errors_optimized)} ({len(errors_optimized)/len(X_test):.1%})")

# Check which errors were fixed
fixed_errors = errors_original[errors_original['optimized_decision'] == errors_original['true_label']]
new_errors = errors_optimized[errors_optimized['c_decision'] == errors_optimized['true_label']]

print(f"\n‚úÖ Errors fixed by optimization: {len(fixed_errors)}")
print(f"‚ö†Ô∏è  New errors introduced: {len(new_errors)}")

if len(fixed_errors) > 0:
    print("\nüìù Samples where optimization fixed errors:")
    for idx, row in fixed_errors.head(3).iterrows():
        print(f"\n  Sample {int(row['sample_id'])}:")
        print(f"    True Label: {row['true_label']}")
        print(f"    Original: {row['c_decision']} (score: {row['c_final_score']})")
        print(f"    Optimized: {row['optimized_decision']} (score: {row['optimized_score']:.1f})")

if len(new_errors) > 0:
    print("\nüìù Samples where optimization introduced errors:")
    for idx, row in new_errors.head(3).iterrows():
        print(f"\n  Sample {int(row['sample_id'])}:")
        print(f"    True Label: {row['true_label']}")
        print(f"    Original: {row['c_decision']} (score: {row['c_final_score']})")
        print(f"    Optimized: {row['optimized_decision']} (score: {row['optimized_score']:.1f})")

# ============================================================
# 5. THRESHOLD SENSITIVITY ANALYSIS
# ============================================================
print("\n" + "="*60)
print("üéöÔ∏è THRESHOLD SENSITIVITY ANALYSIS")
print("="*60)

# Analyze performance around optimized threshold
threshold_range = np.arange(threshold_opt - 10, threshold_opt + 11, 2)
threshold_results = []

for thresh in threshold_range:
    predictions = (X_test['optimized_score'] >= thresh).astype(int)
    true_binary = (X_test['true_label'] == 'Good Fit').astype(int)

    accuracy = (predictions == true_binary).mean()

    # Calculate precision, recall, f1
    tp = ((true_binary == 1) & (predictions == 1)).sum()
    fp = ((true_binary == 0) & (predictions == 1)).sum()
    tn = ((true_binary == 0) & (predictions == 0)).sum()
    fn = ((true_binary == 1) & (predictions == 0)).sum()

    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

    threshold_results.append({
        'threshold': thresh,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1
    })

threshold_df = pd.DataFrame(threshold_results)

# Find best threshold in this range
best_by_f1 = threshold_df.loc[threshold_df['f1'].idxmax()]
best_by_acc = threshold_df.loc[threshold_df['accuracy'].idxmax()]

print(f"\nüìä Threshold Sensitivity (around {threshold_opt}):")
print(f"  Current optimized threshold: {threshold_opt}")
print(f"  Best by F1-Score: {best_by_f1['threshold']} (F1: {best_by_f1['f1']:.3f})")
print(f"  Best by Accuracy: {best_by_acc['threshold']} (Acc: {best_by_acc['accuracy']:.3f})")

# Visualization
fig2, axes2 = plt.subplots(2, 2, figsize=(15, 10))

# Threshold vs Metrics
axes2[0,0].plot(threshold_df['threshold'], threshold_df['accuracy'], 'b-', label='Accuracy', linewidth=2)
axes2[0,0].plot(threshold_df['threshold'], threshold_df['f1'], 'g-', label='F1-Score', linewidth=2)
axes2[0,0].axvline(threshold_opt, color='r', linestyle='--', alpha=0.7, label=f'Optimized ({threshold_opt})')
axes2[0,0].axvline(best_by_f1['threshold'], color='orange', linestyle='--', alpha=0.7,
                   label=f'Best F1 ({best_by_f1["threshold"]})')
axes2[0,0].set_xlabel('Threshold')
axes2[0,0].set_ylabel('Score')
axes2[0,0].set_title('Performance vs Threshold')
axes2[0,0].legend()
axes2[0,0].grid(True, alpha=0.3)

# Precision-Recall tradeoff
axes2[0,1].plot(threshold_df['recall'], threshold_df['precision'], 'b-', linewidth=2)
axes2[0,1].scatter(best_by_f1['recall'], best_by_f1['precision'], color='red', s=100,
                   label=f'Best F1 (thresh={best_by_f1["threshold"]})')
axes2[0,1].scatter(threshold_df[threshold_df['threshold'] == threshold_opt]['recall'].values[0],
                   threshold_df[threshold_df['threshold'] == threshold_opt]['precision'].values[0],
                   color='green', s=100, label=f'Optimized (thresh={threshold_opt})')
axes2[0,1].set_xlabel('Recall')
axes2[0,1].set_ylabel('Precision')
axes2[0,1].set_title('Precision-Recall Curve')
axes2[0,1].legend()
axes2[0,1].grid(True, alpha=0.3)

# Score distribution with thresholds
axes2[1,0].hist(X_test[X_test['true_label'] == 'Good Fit']['optimized_score'],
                alpha=0.5, label='Good Fit', bins=20, density=True)
axes2[1,0].hist(X_test[X_test['true_label'] == 'No Fit']['optimized_score'],
                alpha=0.5, label='No Fit', bins=20, density=True)
axes2[1,0].axvline(threshold_opt, color='r', linestyle='--', alpha=0.7,
                   label=f'Optimized ({threshold_opt})')
axes2[1,0].axvline(best_by_f1['threshold'], color='orange', linestyle='--', alpha=0.7,
                   label=f'Best F1 ({best_by_f1["threshold"]})')
axes2[1,0].set_xlabel('Optimized Score')
axes2[1,0].set_ylabel('Density')
axes2[1,0].set_title('Score Distribution by True Label')
axes2[1,0].legend()
axes2[1,0].grid(True, alpha=0.3)

# Error rates
false_pos_rate = []
false_neg_rate = []
for thresh in threshold_range:
    preds = (X_test['optimized_score'] >= thresh).astype(int)
    true_bin = (X_test['true_label'] == 'Good Fit').astype(int)

    fp = ((true_bin == 0) & (preds == 1)).sum()
    fn = ((true_bin == 1) & (preds == 0)).sum()
    tn = ((true_bin == 0) & (preds == 0)).sum()
    tp = ((true_bin == 1) & (preds == 1)).sum()

    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0

    false_pos_rate.append(fpr)
    false_neg_rate.append(fnr)

axes2[1,1].plot(threshold_range, false_pos_rate, 'r-', label='False Positive Rate', linewidth=2)
axes2[1,1].plot(threshold_range, false_neg_rate, 'b-', label='False Negative Rate', linewidth=2)
axes2[1,1].axvline(threshold_opt, color='black', linestyle='--', alpha=0.7,
                   label=f'Optimized ({threshold_opt})')
axes2[1,1].set_xlabel('Threshold')
axes2[1,1].set_ylabel('Error Rate')
axes2[1,1].set_title('Error Rates vs Threshold')
axes2[1,1].legend()
axes2[1,1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ============================================================
# 6. FINAL CONFIGURATION DECISION
# ============================================================
print("\n" + "="*60)
print("üéØ FINAL CONFIGURATION DECISION")
print("="*60)

# Decide final threshold based on validation
if abs(best_by_f1['threshold'] - threshold_opt) <= 3:
    final_threshold = threshold_opt  # Close enough, use optimized
    print(f"\n‚úÖ Keeping optimized threshold: {final_threshold}")
    print(f"   (Best F1 threshold {best_by_f1['threshold']} is within 3 points)")
else:
    final_threshold = best_by_f1['threshold']
    print(f"\nüîÑ Adjusting to best F1 threshold: {final_threshold}")
    print(f"   (Significantly better than optimized {threshold_opt})")

# Update predictions with final threshold
X_test['final_decision'] = X_test['optimized_score'].apply(
    lambda x: 'Good Fit' if x >= final_threshold else 'No Fit'
)

final_accuracy = (X_test['final_decision'] == X_test['true_label']).mean()

print(f"\nüìä Final Configuration Performance:")
print(f"  Threshold: {final_threshold}")
print(f"  Weights: {weights_opt}")
print(f"  Test Accuracy: {final_accuracy:.2%}")

# Compare all configurations
print("\nüìà Performance Evolution:")
print(f"  Original Pipeline C: {original_correct:.2%}")
print(f"  Initial Optimized: {optimized_correct:.2%}")
print(f"  Final Validated: {final_accuracy:.2%}")

improvement_from_original = final_accuracy - original_correct
print(f"\nüéØ Total Improvement from Original: {improvement_from_original:+.2%}")

# ============================================================
# 7. CREATE FINAL CONFIGURATION FILE
# ============================================================
print("\n" + "="*60)
print("üíæ CREATING FINAL CONFIGURATION")
print("="*60)

# Create final configuration
final_config = {
    "metadata": {
        "created_date": pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S"),
        "dataset_size": len(df_clean),
        "test_set_size": len(X_test),
        "validation_accuracy": float(final_accuracy)
    },
    "configuration": {
        "threshold": int(final_threshold),
        "weights": {k: float(v) for k, v in weights_opt.items()},
        "use_weighted_scoring": has_aspect_scores
    },
    "performance": {
        "test_accuracy": float(final_accuracy),
        "improvement_from_original": float(improvement_from_original),
        "original_accuracy": float(original_correct)
    },
    "validation_results": {
        "best_threshold_by_f1": int(best_by_f1['threshold']),
        "best_threshold_by_accuracy": int(best_by_acc['threshold']),
        "threshold_sensitivity_range": {
            "min": int(threshold_range.min()),
            "max": int(threshold_range.max())
        }
    }
}

# Save final configuration
with open('pipeline_c_final_config.json', 'w') as f:
    json.dump(final_config, f, indent=2)

print("‚úÖ Final configuration saved to: pipeline_c_final_config.json")

# Save test results with predictions
X_test_results = X_test[['sample_id', 'true_label',
                         'c_decision', 'c_final_score',
                         'optimized_decision', 'optimized_score',
                         'final_decision']].copy()

X_test_results.to_csv('pipeline_c_validation_results.csv', index=False)
print("‚úÖ Validation results saved to: pipeline_c_validation_results.csv")

# ============================================================
# 11. STATISTICAL TESTS
# ============================================================
print("\nüìä Statistical Tests")

from scipy import stats

# Test if pipeline accuracies are significantly different
accuracy_scores = {
    'A': df_clean['a_correct'].values,
    'B': df_clean['b_correct'].values,
    'C': df_clean['c_correct'].values
}

# Paired t-tests
print("\nüî¨ Paired t-tests between pipelines:")
pairs = [('A', 'B'), ('A', 'C'), ('B', 'C')]
for pipe1, pipe2 in pairs:
    t_stat, p_value = stats.ttest_rel(accuracy_scores[pipe1], accuracy_scores[pipe2])
    print(f"  {pipe1} vs {pipe2}: t={t_stat:.3f}, p={p_value:.4f}")

# McNemar's test for binary classifications
from statsmodels.stats.contingency_tables import mcnemar

print("\nüî¨ McNemar's test for decision agreement:")
for pipe1, pipe2 in pairs:
    # Create contingency table
    table = pd.crosstab(df_clean[f'{pipe1.lower()}_decision'],
                        df_clean[f'{pipe2.lower()}_decision'])

    # Run McNemar's test
    result = mcnemar(table, exact=False, correction=True)
    print(f"  {pipe1} vs {pipe2}: œá¬≤={result.statistic:.3f}, p={result.pvalue:.4f}")

# ============================================================
# 12. EXPORT RESULTS
# ============================================================
print("\nüíæ Exporting results...")

# Save cleaned data with metrics
df_clean.to_csv('analysis_results_with_metrics.csv', index=False)
print("‚úÖ Saved: analysis_results_with_metrics.csv")

# Save detailed metrics
metrics_df.to_csv('pipeline_detailed_metrics.csv', index=False)
print("‚úÖ Saved: pipeline_detailed_metrics.csv")

# Generate summary report
with open('analysis_summary.txt', 'w') as f:
    f.write("PIPELINE EVALUATION ANALYSIS SUMMARY\n")
    f.write("="*50 + "\n\n")

    f.write(f"Dataset: {len(df_clean)} samples\n")
    f.write(f"True Label Distribution:\n")
    f.write(str(df_clean['true_label'].value_counts()) + "\n\n")

    f.write("Pipeline Performance:\n")
    for _, row in metrics_df.iterrows():
        f.write(f"{row['Pipeline']}:\n")
        f.write(f"  Accuracy: {row['Accuracy']:.2%}\n")
        f.write(f"  Precision: {row['Precision']:.2%}\n")
        f.write(f"  Recall: {row['Recall']:.2%}\n")
        f.write(f"  F1-Score: {row['F1_Score']:.2%}\n\n")

    f.write(f"Best Pipeline by Accuracy: {metrics_df.loc[metrics_df['Accuracy'].idxmax(), 'Pipeline']}\n")
    f.write(f"Best Pipeline by F1-Score: {metrics_df.loc[metrics_df['F1_Score'].idxmax(), 'Pipeline']}\n\n")

    f.write(f"Pipeline Agreement:\n")
    f.write(f"  All 3 agree: {df_clean['all_agree'].mean():.1%}\n")
    f.write(f"  A & B agree: {df_clean['ab_agree'].mean():.1%}\n")
    f.write(f"  A & C agree: {df_clean['ac_agree'].mean():.1%}\n")
    f.write(f"  B & C agree: {df_clean['bc_agree'].mean():.1%}\n")

print("‚úÖ Saved: analysis_summary.txt")

print("\n" + "="*50)
print("ANALYSIS COMPLETE! üéâ")
print("="*50)
print("\nüìÅ Generated files:")
print("  - analysis_results_with_metrics.csv")
print("  - pipeline_detailed_metrics.csv")
print("  - analysis_summary.txt")
print("\nüìä Key insights saved to summary file.")

"""## Batch"""

def run_evaluation(df, limit=None):
    records = []

    n = len(df) if limit is None else min(limit, len(df))

    for i in tqdm(range(n)):
        # r_text = df.iloc[i]["resume_text"]
        # j_text = df.iloc[i]["job_description_text"]
        # true_label = df.iloc[i]["label"]

        r_text = df.iloc[i]["resume_summary"]
        j_text = df.iloc[i]["jd_summary"]
        true_label = df.iloc[i]["true_label"]

        try:
            out = evaluate_pair(r_text, j_text, extract=False)
        except Exception as e:
            print("Error on index", i, e)
            out = {
                "resume_summary": "",
                "jd_summary": "",
                "scores": {},
                "final_score": 0,
                "decision": "No Fit",
                "overall_reason": ""
            }

        records.append({
            "index": i,
            "true_label": true_label,
            "pred_label": out["decision"],
            "final_score": out["final_score"],
            "overall_reason": out["overall_reason"],
            "resume_summary": out["resume_summary"],
            "jd_summary": out["jd_summary"],
            **out["scores"],
        })

    return pd.DataFrame(records)



df_test = pd.read_csv('sample_test.csv')
print(len(df_test))
df_test.head(2)

results = run_evaluation(df_res)

filename = "evaluation_results.csv"
# Jika file sudah ada, tambahkan suffix angka
if os.path.exists(filename):
    base, ext = os.path.splitext(filename)
    counter = 1
    new_filename = f"{base}_{counter}{ext}"
    while os.path.exists(new_filename):
        counter += 1
        new_filename = f"{base}_{counter}{ext}"
    filename = new_filename

# Simpan dataframe
results.to_csv(filename, index=False)
print(f"Saved results to: {filename}")

print(results.head(2))

results = pd.read_csv('eval_all_pipelines.csv')
results.head()

def normalize_label(label):
    label = str(label).lower()
    if "good" in label:
        return "Good Fit"
    if "potential" in label:
        return "Potential Fit"
    return "No Fit"

results["true_norm"] = results["true_label"].apply(normalize_label)
results["pred_norm"] = results["pred_label"].apply(normalize_label)

results["final_score"] = results["C_score"]

results["reason"] = results["C_overall_reason"]

results.head()

"""## F1, Accuracy, Precision, Recall"""

y_true = results["true_norm"]
y_pred = results["pred_norm"]

acc = accuracy_score(y_true, y_pred)
f1_macro = f1_score(y_true, y_pred, average="macro")
f1_weighted = f1_score(y_true, y_pred, average="weighted")
precision = precision_score(y_true, y_pred, average="macro")
recall = recall_score(y_true, y_pred, average="macro")

print("=== MAIN METRICS ===")
print("Accuracy:", acc)
print("Precision (macro):", precision)
print("Recall (macro):", recall)
print("F1 Macro:", f1_macro)
print("F1 Weighted:", f1_weighted)

"""## Confusion Matrix"""

labels=["No Fit", "Potential Fit", "Good Fit"]
cm = confusion_matrix(y_true, y_pred, labels=["No Fit", "Potential Fit", "Good Fit"])
print("=== Confusion Matrix ===")
print(cm)

plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix Visualization')
plt.show()

print(classification_report(y_true, y_pred))

print(results[results['true_norm'] == 'Good Fit']['final_score'].describe())
print(results[results['true_norm'] == 'No Fit']['final_score'].describe())
print(results[results['true_norm'] == 'Potential Fit']['final_score'].describe())

true_rank = results["true_norm"].map({"No Fit":0, "Potential Fit":1, "Good Fit":2})
pred_score = results["pred_norm"].map({"No Fit":0, "Potential Fit":1, "Good Fit":2})

from scipy.stats import pearsonr
r, p = pearsonr(true_rank, pred_score)

print("Pearson R:", r)
print("p-value:", p)

import seaborn as sns
import matplotlib.pyplot as plt

"""##Score Distribution per Label"""

plt.figure(figsize=(8,5))
sns.boxplot(data=results, x="true_norm", y="final_score")
plt.title("Final Score Distribution per True Label")
plt.show()

"""## Heatmap Korelasi Aspek"""

score_cols = [
    "Skill_Match","Experience_Alignment",
    "Domain_Fit","Tool_Tech_Match",
    "Seniority_Alignment","Education_Alignment"
]

for aspect in score_cols:
    print(f"{aspect}: Mean = {results[aspect].mean():.1f}, Std = {results[aspect].std():.1f}")

corr = results[score_cols].corr()

plt.figure(figsize=(8,6))
sns.heatmap(corr, annot=True, cmap="coolwarm", vmin=-1, vmax=1)
plt.title("Correlation of Multi-Aspect Scores")
plt.show()

"""## Plot overall score histogram"""

plt.figure(figsize=(7,4))
sns.histplot(results["final_score"], bins=20, kde=True)
plt.title("Distribution of Final Scores")
plt.show()

plt.hist(results["pred_norm"].dropna(), bins=20)
plt.title("Distribution of LLM Fit Scores")
plt.xlabel("Score")
plt.ylabel("Count")
plt.show()

"""## ROC-AUC One-vs-Rest"""

from sklearn.preprocessing import label_binarize

label_list = ["No Fit", "Potential Fit", "Good Fit"]
y_true_num = label_binarize(results["true_norm"], classes=label_list)

# For ROC-AUC, we need probability-like scores.
# We will use final_score normalized to [0,1].
results["score_norm"] = results["final_score"] / 100.0

from sklearn.metrics import roc_auc_score
import numpy as np

# Build probability matrix (pseudo-probabilities)
p_good = results["score_norm"].values
p_not = 1 - p_good
p_potential = np.clip(1 - np.abs(results["score_norm"] - 0.5)*2, 0, 1)

Y_pred = np.vstack([p_not, p_potential, p_good]).T  # shape: N x 3

roc_auc = roc_auc_score(y_true_num, Y_pred, average="macro", multi_class="ovr")
roc_auc

"""## ROC-AUC Multiclass (One-vs-Rest + Macro-AUC)"""

label_map = {"No Fit":0, "Potential Fit":1, "Good Fit":2}

y_true_rank = results["true_norm"].map(label_map)

from sklearn.preprocessing import label_binarize
true_bin = label_binarize(
    y_true_rank,
    classes=[0,1,2]
)

y_scores = np.array(results["final_score"]) / 100.0
# duplicate into 3 columns artificially for OvR requirement
scores_multi = np.column_stack([
    1 - y_scores,   # inverse score ‚Üí class 0
    y_scores * 0.5, # intermediate ‚Üí class 1
    y_scores        # predicted ‚Üí class 2
])

from sklearn.metrics import roc_auc_score

roc_macro = roc_auc_score(true_bin, scores_multi, average="macro")
roc_weighted = roc_auc_score(true_bin, scores_multi, average="weighted")

print("ROC-AUC Macro:", roc_macro)
print("ROC-AUC Weighted:", roc_weighted)

"""# Task
Perform stratified sampling on the `df_test` DataFrame to create a new DataFrame named `sampled_df_test` containing 280 records. The sampling should be based on the 'label' column with the following distribution: 50% 'No Fit', 25% 'Potential Fit', and 25% 'Good Fit'. Save the `sampled_df_test` DataFrame to a CSV file named `sampled_df_test.csv`. Finally, run the `run_evaluation` function using the `sampled_df_test` DataFrame.

## Perform Stratified Sampling

### Subtask:
Create a stratified random sample of 280 records from `df_test` based on the 'label' column, with 50% 'No Fit', 25% 'Potential Fit', and 25% 'Good Fit'.

**Reasoning**:
First, I'll calculate the number of samples needed for each 'label' category based on the specified percentages and total sample size.
"""

# Total target sample size
total_sample_size = 280  # OR: use len(df)
target_distribution = {
    "No Fit": 0.50,
    "Potential Fit": 0.25,
    "Good Fit": 0.25
}

sample_counts = {
    label: int(total_sample_size * percentage)
    for label, percentage in target_distribution.items()
}

print("Sample counts per label category:", sample_counts)

"""**Reasoning**:
Now that the sample counts per label are determined, I will group the `df_test` by 'label' and sample the calculated number of records from each group, then concatenate them to create `sampled_df_test`.


"""

sampled_df_test = pd.DataFrame()
for label, count in sample_counts.items():
    # don't try to sample more than available in the group
    group_df = df_test[df_test['label'] == label]
    actual_count = min(count, len(group_df))
    sampled_group = group_df.sample(n=actual_count, random_state=42)
    sampled_df_test = pd.concat([sampled_df_test, sampled_group])

# Shuffle the sampled DataFrame to mix the categories
sampled_df_test = sampled_df_test.sample(frac=1, random_state=42).reset_index(drop=True)

print("Sampled df_test shape:", sampled_df_test.shape)
print("Sampled df_test distribution:\n", sampled_df_test['label'].value_counts())

# Filter hanya Good Fit dan No Fit
df_binary = df_test[df_test['label'].isin(['Good Fit', 'No Fit'])].copy()

# Hitung distribusi yang ada
print("Original distribution in df_test:")
print(df_test['label'].value_counts())
print(f"\nBinary dataset available: {len(df_binary)} samples")

# Untuk 280 samples, perlu 140 Good Fit dan 140 No Fit
target_counts = {
    "Good Fit": 200,
    "No Fit": 200
}

sampled_df = pd.DataFrame()

for label, count in target_counts.items():
    group_df = df_binary[df_binary['label'] == label]

    if len(group_df) >= count:
        sampled_group = group_df.sample(n=count, random_state=42)
    else:
        # Jika tidak cukup, ambil semua yang ada
        print(f"Warning: Only {len(group_df)} {label} samples available, taking all")
        sampled_group = group_df.sample(n=len(group_df), random_state=42)

    sampled_df = pd.concat([sampled_df, sampled_group])

# Shuffle
sampled_df = sampled_df.sample(frac=1, random_state=42).reset_index(drop=True)

print(f"\nFinal binary dataset shape: {sampled_df.shape}")
print("Distribution:")
print(sampled_df['label'].value_counts())
print(f"\nGood Fit: {(sampled_df['label'] == 'Good Fit').sum()}")
print(f"No Fit: {(sampled_df['label'] == 'No Fit').sum()}")

# Save sample dataset
sampled_df.to_csv("binary_sample_test.csv", index=False)